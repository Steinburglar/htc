{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88283e85-2a29-4816-b5ec-a8d15613eb1c",
   "metadata": {},
   "source": [
    "# Creating Predictions\n",
    "If you have some HSI images and want to get predictions from one of our pretrained models (or your own models), then we covered you in this notebook. Predictions can be computed based on a folder with HSI images and the `htc inference` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f50cc1a7-971b-40cd-ba89-43c2835257e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from htc import Config, LabelMapping, decompress_file, settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e04bfa34-330b-47f7-b032-290db9e4383f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span><span style=\"color: #808000; text-decoration-color: #808000\">WARNING</span><span style=\"font-weight: bold\">][</span><span style=\"font-style: italic\">htc</span><span style=\"font-weight: bold\">]</span> The environment variable PATH_Tivita_Cat_atlas_kidney_nativ was set to                    <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">Datasets.py:125</span>\n",
       "<span style=\"color: #800080; text-decoration-color: #800080\">/home/lucas/dkfz/htc/tests/test_31may/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">Cat_atlas_kidney_nativ</span> but the path does not exist                 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\u001b[33mWARNING\u001b[0m\u001b[1m]\u001b[0m\u001b[1m[\u001b[0m\u001b[3mhtc\u001b[0m\u001b[1m]\u001b[0m The environment variable PATH_Tivita_Cat_atlas_kidney_nativ was set to                    \u001b[2mDatasets.py:125\u001b[0m\n",
       "\u001b[35m/home/lucas/dkfz/htc/tests/test_31may/\u001b[0m\u001b[95mCat_atlas_kidney_nativ\u001b[0m but the path does not exist                 \u001b[2m               \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Class: MultiPath\n",
       "Root location: /home/lucas/dkfz/htc/tests/test_28may/test_results28may (exists=True)\n",
       "Best location (considering needle /home/lucas/dkfz/htc/tests/test_28may/test_results28may): /home/lucas/dkfz/htc/tests/test_28may/test_results28may (exists=True)\n",
       "All locations:\n",
       "/home/lucas/dkfz/htc/tests/test_28may/test_results28may (exists=True)\n",
       "/home/lucas/dkfz/htc/tutorials/:~/dkfz/htc/tests/test_31may/test_results31may (exists=False)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir = settings.data_dirs.test_dataset28may\n",
    "output_dir = settings.results_dir/ \"predictions\"\n",
    "data_dir\n",
    "settings.results_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88c10e8-cca8-46f9-91e1-3a9f02d55103",
   "metadata": {},
   "source": [
    "> Note: If you want to use your own data and it does not fit into the default structure (e.g. because you have non-Tivita images), they you need to write your [own DataPath class](./CustomDataPath.md) first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1997912-0a42-45e2-b170-d2774a4069ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m[\u001b[0m\u001b[33mWARNING\u001b[0m\u001b[1m]\u001b[0m\u001b[1m[\u001b[0m\u001b[3mhtc\u001b[0m\u001b[1m]\u001b[0m The environment variable                          \u001b[2mDatasets.py:125\u001b[0m\n",
      "PATH_Tivita_Cat_atlas_kidney_nativ was set to                    \u001b[2m               \u001b[0m\n",
      "\u001b[35m/home/lucas/dkfz/htc/tests/test_31may/\u001b[0m\u001b[95mCat_atlas_kidney_nativ\u001b[0m but \u001b[2m               \u001b[0m\n",
      "the path does not exist                                          \u001b[2m               \u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[32mINFO\u001b[0m\u001b[1m]\u001b[0m\u001b[1m[\u001b[0m\u001b[3mhtc\u001b[0m\u001b[1m]\u001b[0m Compute the prediction for \u001b[37m1\u001b[0m images              \u001b[2mrun_inference.py:70\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[32mINFO\u001b[0m\u001b[1m]\u001b[0m\u001b[1m[\u001b[0m\u001b[3mhtc.no_duplicates\u001b[0m\u001b[1m]\u001b[0m Found pretrained run in the local hub  \u001b[2mHTCModel.py:484\u001b[0m\n",
      "dir at                                                           \u001b[2m               \u001b[0m\n",
      "\u001b[35m/home/lucas/.cache/torch/hub/htc_checkpoints/image/\u001b[0m\u001b[95m2022-02-03_22\u001b[0m \u001b[2m               \u001b[0m\n",
      "\u001b[95m-58-44_generated_default_model_comparison\u001b[0m                        \u001b[2m               \u001b[0m\n",
      "INFO: Lightning automatically upgraded your loaded checkpoint from v1.5.8 to v2.2.5. To apply the upgrade to your files permanently, run `python -m lightning.pytorch.utilities.upgrade_checkpoint ../../../.cache/torch/hub/htc_checkpoints/image/2022-02-03_22-58-44_generated_default_model_comparison/fold_P041,P060,P069/epoch=46-dice_metric=0.87.ckpt`\n",
      "\u001b[1m[\u001b[0m\u001b[32mINFO\u001b[0m\u001b[1m]\u001b[0m\u001b[1m[\u001b[0m\u001b[3mlightning.pytorch.utilities.migration.utils\u001b[0m\u001b[1m]\u001b[0m Lightning       \u001b[2mutils.py:154\u001b[0m\n",
      "automatically upgraded your loaded checkpoint from v1.\u001b[37m5.8\u001b[0m to        \u001b[2m            \u001b[0m\n",
      "v2.\u001b[37m2.5\u001b[0m. To apply the upgrade to your files permanently, run `python \u001b[2m            \u001b[0m\n",
      "-m lightning.pytorch.utilities.upgrade_checkpoint                   \u001b[2m            \u001b[0m\n",
      "..\u001b[35m/../../.cache/torch/hub/htc_checkpoints/image/2022-02-03_22-58-44\u001b[0m \u001b[2m            \u001b[0m\n",
      "\u001b[35m_generated_default_model_comparison/\u001b[0m\u001b[95mfold_P041\u001b[0m,P060,P069/\u001b[33mepoch\u001b[0m=\u001b[37m46\u001b[0m-\u001b[33mdi\u001b[0m \u001b[2m            \u001b[0m\n",
      "\u001b[33mce_metric\u001b[0m=\u001b[37m0\u001b[0m\u001b[37m.87\u001b[0m.ckpt`                                                \u001b[2m            \u001b[0m\n",
      "INFO: Lightning automatically upgraded your loaded checkpoint from v1.5.8 to v2.2.5. To apply the upgrade to your files permanently, run `python -m lightning.pytorch.utilities.upgrade_checkpoint ../../../.cache/torch/hub/htc_checkpoints/image/2022-02-03_22-58-44_generated_default_model_comparison/fold_P044,P050,P059/epoch=70-dice_metric=0.90.ckpt`\n",
      "\u001b[1m[\u001b[0m\u001b[32mINFO\u001b[0m\u001b[1m]\u001b[0m\u001b[1m[\u001b[0m\u001b[3mlightning.pytorch.utilities.migration.utils\u001b[0m\u001b[1m]\u001b[0m Lightning       \u001b[2mutils.py:154\u001b[0m\n",
      "automatically upgraded your loaded checkpoint from v1.\u001b[37m5.8\u001b[0m to        \u001b[2m            \u001b[0m\n",
      "v2.\u001b[37m2.5\u001b[0m. To apply the upgrade to your files permanently, run `python \u001b[2m            \u001b[0m\n",
      "-m lightning.pytorch.utilities.upgrade_checkpoint                   \u001b[2m            \u001b[0m\n",
      "..\u001b[35m/../../.cache/torch/hub/htc_checkpoints/image/2022-02-03_22-58-44\u001b[0m \u001b[2m            \u001b[0m\n",
      "\u001b[35m_generated_default_model_comparison/\u001b[0m\u001b[95mfold_P044\u001b[0m,P050,P059/\u001b[33mepoch\u001b[0m=\u001b[37m70\u001b[0m-\u001b[33mdi\u001b[0m \u001b[2m            \u001b[0m\n",
      "\u001b[33mce_metric\u001b[0m=\u001b[37m0\u001b[0m\u001b[37m.90\u001b[0m.ckpt`                                                \u001b[2m            \u001b[0m\n",
      "INFO: Lightning automatically upgraded your loaded checkpoint from v1.5.8 to v2.2.5. To apply the upgrade to your files permanently, run `python -m lightning.pytorch.utilities.upgrade_checkpoint ../../../.cache/torch/hub/htc_checkpoints/image/2022-02-03_22-58-44_generated_default_model_comparison/fold_P045,P061,P071/epoch=75-dice_metric=0.84.ckpt`\n",
      "\u001b[1m[\u001b[0m\u001b[32mINFO\u001b[0m\u001b[1m]\u001b[0m\u001b[1m[\u001b[0m\u001b[3mlightning.pytorch.utilities.migration.utils\u001b[0m\u001b[1m]\u001b[0m Lightning       \u001b[2mutils.py:154\u001b[0m\n",
      "automatically upgraded your loaded checkpoint from v1.\u001b[37m5.8\u001b[0m to        \u001b[2m            \u001b[0m\n",
      "v2.\u001b[37m2.5\u001b[0m. To apply the upgrade to your files permanently, run `python \u001b[2m            \u001b[0m\n",
      "-m lightning.pytorch.utilities.upgrade_checkpoint                   \u001b[2m            \u001b[0m\n",
      "..\u001b[35m/../../.cache/torch/hub/htc_checkpoints/image/2022-02-03_22-58-44\u001b[0m \u001b[2m            \u001b[0m\n",
      "\u001b[35m_generated_default_model_comparison/\u001b[0m\u001b[95mfold_P045\u001b[0m,P061,P071/\u001b[33mepoch\u001b[0m=\u001b[37m75\u001b[0m-\u001b[33mdi\u001b[0m \u001b[2m            \u001b[0m\n",
      "\u001b[33mce_metric\u001b[0m=\u001b[37m0\u001b[0m\u001b[37m.84\u001b[0m.ckpt`                                                \u001b[2m            \u001b[0m\n",
      "INFO: Lightning automatically upgraded your loaded checkpoint from v1.5.8 to v2.2.5. To apply the upgrade to your files permanently, run `python -m lightning.pytorch.utilities.upgrade_checkpoint ../../../.cache/torch/hub/htc_checkpoints/image/2022-02-03_22-58-44_generated_default_model_comparison/fold_P047,P049,P070/epoch=52-dice_metric=0.85.ckpt`\n",
      "\u001b[1m[\u001b[0m\u001b[32mINFO\u001b[0m\u001b[1m]\u001b[0m\u001b[1m[\u001b[0m\u001b[3mlightning.pytorch.utilities.migration.utils\u001b[0m\u001b[1m]\u001b[0m Lightning       \u001b[2mutils.py:154\u001b[0m\n",
      "automatically upgraded your loaded checkpoint from v1.\u001b[37m5.8\u001b[0m to        \u001b[2m            \u001b[0m\n",
      "v2.\u001b[37m2.5\u001b[0m. To apply the upgrade to your files permanently, run `python \u001b[2m            \u001b[0m\n",
      "-m lightning.pytorch.utilities.upgrade_checkpoint                   \u001b[2m            \u001b[0m\n",
      "..\u001b[35m/../../.cache/torch/hub/htc_checkpoints/image/2022-02-03_22-58-44\u001b[0m \u001b[2m            \u001b[0m\n",
      "\u001b[35m_generated_default_model_comparison/\u001b[0m\u001b[95mfold_P047\u001b[0m,P049,P070/\u001b[33mepoch\u001b[0m=\u001b[37m52\u001b[0m-\u001b[33mdi\u001b[0m \u001b[2m            \u001b[0m\n",
      "\u001b[33mce_metric\u001b[0m=\u001b[37m0\u001b[0m\u001b[37m.85\u001b[0m.ckpt`                                                \u001b[2m            \u001b[0m\n",
      "INFO: Lightning automatically upgraded your loaded checkpoint from v1.5.8 to v2.2.5. To apply the upgrade to your files permanently, run `python -m lightning.pytorch.utilities.upgrade_checkpoint ../../../.cache/torch/hub/htc_checkpoints/image/2022-02-03_22-58-44_generated_default_model_comparison/fold_P048,P057,P058/epoch=79-dice_metric=0.86.ckpt`\n",
      "\u001b[1m[\u001b[0m\u001b[32mINFO\u001b[0m\u001b[1m]\u001b[0m\u001b[1m[\u001b[0m\u001b[3mlightning.pytorch.utilities.migration.utils\u001b[0m\u001b[1m]\u001b[0m Lightning       \u001b[2mutils.py:154\u001b[0m\n",
      "automatically upgraded your loaded checkpoint from v1.\u001b[37m5.8\u001b[0m to        \u001b[2m            \u001b[0m\n",
      "v2.\u001b[37m2.5\u001b[0m. To apply the upgrade to your files permanently, run `python \u001b[2m            \u001b[0m\n",
      "-m lightning.pytorch.utilities.upgrade_checkpoint                   \u001b[2m            \u001b[0m\n",
      "..\u001b[35m/../../.cache/torch/hub/htc_checkpoints/image/2022-02-03_22-58-44\u001b[0m \u001b[2m            \u001b[0m\n",
      "\u001b[35m_generated_default_model_comparison/\u001b[0m\u001b[95mfold_P048\u001b[0m,P057,P058/\u001b[33mepoch\u001b[0m=\u001b[37m79\u001b[0m-\u001b[33mdi\u001b[0m \u001b[2m            \u001b[0m\n",
      "\u001b[33mce_metric\u001b[0m=\u001b[37m0\u001b[0m\u001b[37m.86\u001b[0m.ckpt`                                                \u001b[2m            \u001b[0m\n",
      "\u001b[2KDataloader \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m  0%\u001b[0m \u001b[36m-:--:--\u001b[0m \u001b[33m0:00:27\u001b[0m\u001b[1m[\u001b[0m\u001b[33mWARNING\u001b[0m\u001b[1m]\u001b[0m\u001b[1m[\u001b[0m\u001b[3mhtc\u001b[0m\u001b[1m]\u001b[0m Found \u001b[37m3315302400\u001b[0m nan values in the            \u001b[2mHTCLightning.py:145\u001b[0m\n",
      "predictions class \u001b[1m(\u001b[0mtensor.shape = \u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[37m1\u001b[0m, \u001b[37m19\u001b[0m, \u001b[37m480\u001b[0m,    \u001b[2m                   \u001b[0m\n",
      "\u001b[37m640\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\u001b[1m)\u001b[0m                                                       \u001b[2m                   \u001b[0m\n",
      "\u001b[2KDataloader \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m  0%\u001b[0m \u001b[36m-:--:--\u001b[0m \u001b[33m0:00:29\u001b[0m\u001b[1m[\u001b[0m\u001b[33mWARNING\u001b[0m\u001b[1m]\u001b[0m\u001b[1m[\u001b[0m\u001b[3mhtc\u001b[0m\u001b[1m]\u001b[0m Found \u001b[37m3315302400\u001b[0m nan values in the            \u001b[2mHTCLightning.py:145\u001b[0m\n",
      "predictions class \u001b[1m(\u001b[0mtensor.shape = \u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[37m1\u001b[0m, \u001b[37m19\u001b[0m, \u001b[37m480\u001b[0m,    \u001b[2m                   \u001b[0m\n",
      "\u001b[37m640\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\u001b[1m)\u001b[0m                                                       \u001b[2m                   \u001b[0m\n",
      "\u001b[2KDataloader \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m  0%\u001b[0m \u001b[36m-:--:--\u001b[0m \u001b[33m0:00:31\u001b[0m\u001b[1m[\u001b[0m\u001b[33mWARNING\u001b[0m\u001b[1m]\u001b[0m\u001b[1m[\u001b[0m\u001b[3mhtc\u001b[0m\u001b[1m]\u001b[0m Found \u001b[37m3315302400\u001b[0m nan values in the            \u001b[2mHTCLightning.py:145\u001b[0m\n",
      "predictions class \u001b[1m(\u001b[0mtensor.shape = \u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[37m1\u001b[0m, \u001b[37m19\u001b[0m, \u001b[37m480\u001b[0m,    \u001b[2m                   \u001b[0m\n",
      "\u001b[37m640\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\u001b[1m)\u001b[0m                                                       \u001b[2m                   \u001b[0m\n",
      "\u001b[2KDataloader \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m  0%\u001b[0m \u001b[36m-:--:--\u001b[0m \u001b[33m0:00:33\u001b[0m\u001b[1m[\u001b[0m\u001b[33mWARNING\u001b[0m\u001b[1m]\u001b[0m\u001b[1m[\u001b[0m\u001b[3mhtc\u001b[0m\u001b[1m]\u001b[0m Found \u001b[37m3315302400\u001b[0m nan values in the            \u001b[2mHTCLightning.py:145\u001b[0m\n",
      "predictions class \u001b[1m(\u001b[0mtensor.shape = \u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[37m1\u001b[0m, \u001b[37m19\u001b[0m, \u001b[37m480\u001b[0m,    \u001b[2m                   \u001b[0m\n",
      "\u001b[37m640\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\u001b[1m)\u001b[0m                                                       \u001b[2m                   \u001b[0m\n",
      "\u001b[2KDataloader \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m  0%\u001b[0m \u001b[36m-:--:--\u001b[0m \u001b[33m0:00:34\u001b[0m\u001b[1m[\u001b[0m\u001b[33mWARNING\u001b[0m\u001b[1m]\u001b[0m\u001b[1m[\u001b[0m\u001b[3mhtc\u001b[0m\u001b[1m]\u001b[0m Found \u001b[37m3315302400\u001b[0m nan values in the            \u001b[2mHTCLightning.py:145\u001b[0m\n",
      "predictions class \u001b[1m(\u001b[0mtensor.shape = \u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[37m1\u001b[0m, \u001b[37m19\u001b[0m, \u001b[37m480\u001b[0m,    \u001b[2m                   \u001b[0m\n",
      "\u001b[37m640\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\u001b[1m)\u001b[0m                                                       \u001b[2m                   \u001b[0m\n",
      "\u001b[2KDataloader \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m100%\u001b[0m \u001b[36m0:00:00\u001b[0m \u001b[33m0:00:37\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!htc inference --input-dir $data_dir/subjects/P086/2021_04_15_09_22_02 --output-dir $output_dir --model image --run-folder 2022-02-03_22-58-44_generated_default_model_comparison\n",
    "assert _exit_code == 0, \"Inference was not successful\"  # noqa: F821................................................................................................."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a82bc5e-b48b-4aa5-9dbc-ac300e6724f4",
   "metadata": {},
   "source": [
    "This command searches for all HSI images in the given input directory, computes a prediction using the specified trained model (will also be downloaded if not available) and stores the result in the given output directory. You can use any of the pretrained models here.\n",
    "\n",
    "In this example case, there is only one output image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "059251ab-f15f-436b-bca5-deeac44a29ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('/home/lucas/dkfz/htc/tests/test_28may/test_results28may/predictions/image'),\n",
       " PosixPath('/home/lucas/dkfz/htc/tests/test_28may/test_results28may/predictions/image/2022-02-03_22-58-44_generated_default_model_comparison'),\n",
       " PosixPath('/home/lucas/dkfz/htc/tests/test_28may/test_results28may/predictions/image/2022-02-03_22-58-44_generated_default_model_comparison/0202-00118#2021_04_15_09_22_02.blosc'),\n",
       " PosixPath('/home/lucas/dkfz/htc/tests/test_28may/test_results28may/predictions/image/2022-02-03_22-58-44_generated_default_model_comparison/0202-00118#2021_04_15_09_22_02.html'),\n",
       " PosixPath('/home/lucas/dkfz/htc/tests/test_28may/test_results28may/predictions/image/2022-02-03_22-58-44_generated_default_model_comparison/config.json')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(output_dir.rglob(\"*\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14db9ec1-c7c9-493e-b4af-b3d77f90e22e",
   "metadata": {},
   "source": [
    "Per default, this includes the predicted labels (stored in the blosc file), a visualization of the prediction (HTML file) and the config which was used for computing the predictions (which again is based on the config of the trained model). You can open the HTML file directly with any browser. The labels can be read with the `decompress_file()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be9a00b0-7bf0-4237-ae5d-2840ecc38cd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = decompress_file(sorted(output_dir.rglob(\"*.blosc\"))[0])\n",
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a64b0c7e-89fe-403e-99d8-8d349ff24443",
   "metadata": {},
   "source": [
    "The config can, for example, be used to recover the original label names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "45244a8c-281f-4a29-b1ac-8f304877c581",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "background: 307200 pixels\n"
     ]
    }
   ],
   "source": [
    "config = Config(sorted(output_dir.rglob(\"config.json\"))[0])\n",
    "mapping = LabelMapping.from_config(config)\n",
    "\n",
    "for l, c in zip(*np.unique(labels, return_counts=True)):\n",
    "    print(f\"{mapping.index_to_name(l)}: {c} pixels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b30945-dadb-4cf6-9036-b9aee922ad3d",
   "metadata": {},
   "source": [
    "> Note: if you need the softmax values instead of the label indices of the prediction, add the `--predictions-type softmax` switch to `htc inference`. However, be aware that this requires much more disk space (around 17 MiB instead of 2 KiB per image)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
