{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tissue Atlas Classification Training Walkthrough\n",
    "This notebook will walk a user through using the Atlas compatible htc for training their own organ classification (not segmentation) model based on spectral analysis. There is another, similar notebook for training a segmentation model, named \"SegmentationTraining.ipynb\" If you have not yet, please read the Setup tutorial for important information.\n",
    "Start with necessary inputs and define path to your dataset_settings .json. The tutorial is written with a very small dataset (2 pigs) called \"HeiPorSpectral_mod\". Replace relevant directory paths / names with the names to your own dataset and json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "/omics/groups/OE0645/internal/data/htcdata/medium_test/external/intermediates\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from htc.tissue_atlas.settings_atlas import SettingsAtlas\n",
    "import pandas as pd\n",
    "from IPython.display import JSON\n",
    "from typing import TYPE_CHECKING, Any, Callable, Union, Self\n",
    "from htc import (\n",
    "    Config,\n",
    "    DataPath,\n",
    "    DataSpecification,\n",
    "    MetricAggregation,\n",
    "    SpecsGeneration,\n",
    "    create_class_scores_figure,\n",
    "    settings,\n",
    ")\n",
    "from htc.models.data.SpecsGenerationAtlas import SpecsGenerationAtlas\n",
    "\n",
    "intermediates_dir = settings.intermediates_dirs.external\n",
    "print(intermediates_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can specify important parameters for your training run, such as fold, train/test split, etc. replace the values in the following code block with the values of your choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TO DO:\n",
    "#filter by existence of .txt file next to hypergui within timestamp folder -- lets you know that its ok\n",
    "#add batch size\n",
    "#add epoch length\n",
    "#add batch randomization conditions\n",
    "\n",
    "filter_txt = lambda p: p.contains_txt()\n",
    "filters = [filter_txt] #list of callable filter functions, can be a variety of things\n",
    "annotation_name = 'annotator1' #name of annotators to be used\n",
    "test_ratio = 0 #ratio of images to be saved as test, i.e, not ued in any training. should be float between 0.0 and 1.0\n",
    "n_folds = 3 #number of folds to make in the training data. training data (not test data) will be randomly split into n_folds different groups\n",
    "#for each \"fold\", the network will train a model with one of the groups as validation and all the other groups as training data.  \n",
    "seed = None #optional parameter that interacts with the random grouping of the folding operation. For a different fold upon every function call, set = None.\n",
    "# for a consistent fold, set seed to a number of your choice, e.g. seed = 42\n",
    "name = \"Atlas\" #name of a json file created in the following code block, that gets stored in the parent directory of this notebook. name it something simple and descriptive\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/omics/groups/OE0645/internal/data/htcdata/medium_test/external\n",
      "['P160_OP124_2023_06_28_Experiment1', 'P162_OP126_2023_07_06_Experiment1', 'P163_OP127_2023_07_12_Experiment1']\n"
     ]
    }
   ],
   "source": [
    "tutorial_dir = Path().absolute()\n",
    "external = settings.external_dir.external['path_dataset']#need brackets to acess the path, because settings.external_dir.external is a dictionary cointaing info about the external_dir.\n",
    "#settings.external_dir is an object containing all the different external directories: in our case, there should always just be one with shortcut \"external\"\n",
    "print(external)\n",
    "specs_path = external/'data' / name\n",
    "SpecsGenerationAtlas(intermediates_dir,\n",
    "                filters = filters,\n",
    "                annotation_name = annotation_name,\n",
    "                test_ratio = test_ratio,\n",
    "                n_folds = n_folds,\n",
    "                seed = seed,\n",
    "                name = name,\n",
    "                ).generate_dataset(external / 'data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lightning Class\n",
    "\n",
    "Next step is to choose/build our lightning class. The Lightning class (as in Pytorch Lightning) performs many aspects of managing training, and can b customized by creating your own child class. most notably, the Lightning class allows you to specify your Loss function.\n",
    "\n",
    "For this walkthrough, we will use the htc default \"LightingImage\" class, which is their default class for training on full images (as opposed to patches, pixels, or superpixels). This calculates loss as a weighted average of Dice loss and Cross-Entropy loss. See the htc \"networkTraining\" tutorial for more info on the lightning class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config\n",
    "The last step before training is to create our configuration file. This file is also a json that contains important metadata, and it is used by the training process itself to configure training hyperparameters, like batch size and transformations. We will use the htc's Config ***class*** to write the config ***json***\n",
    "\n",
    "The following Code block will write the config json for you. By default, it will store the config.json file in the same directory as your dataset_settings json.\n",
    "\n",
    "The following code block is still set up for tutorial use, not production use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assign training hyperparameters\n",
    "max_epochs = 1 #this can be whatever you want\n",
    "batch_size = 2 #this is the number of SUBJECTS, rather than images, in each batch. The loader is designed to sort batches by subject\n",
    "#CANNOT BE BATCH SIZE 1, breaks the batch norm steps?\n",
    "#default batch size is 3 subjects\n",
    "shuffle = True #this tells the batch generator to retrieve random, different, batches on every epoch.\n",
    "#True causes it to be random, False will leave same batches across epochs. \n",
    "num_workers = \"auto\" #how many dataloading \"worker\" subprocesses to start. The optimal amount for fast loading is highly dependant on your system\n",
    "#you can experiment on low-epoch runs to see what num_workers maximizes your training speed. \n",
    "#left to implement: specialized sampling practices? such as guaranteeing even organ distribution across classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load up settingsAtlas object, which contains among other properties a label mapping, that can be used by the training\n",
    "Settings_Atlas = SettingsAtlas()\n",
    "label_mapping_train = Settings_Atlas.label_mapping\n",
    "#this part shpuld be modified, and I should probably create my own settings once i have a clearer view of what alex wants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pathlib.PosixPath'>\n",
      "LabelMapping(stomach=0, small_bowel=1, colon=2, liver=3, gallbladder=4, pancreas=5, kidney=6, lung=7, heart=8, cartilage=9, bile_fluid=10, kidney_with_Gerotas_fascia=11, major_vein=12, peritoneum=13, muscle=14, skin=15, bone=16, omentum=17, bladder=18, spleen=19, uro_conduit=20)\n"
     ]
    }
   ],
   "source": [
    "config = Config(\"/home/l328r/htc/htc/tissue_atlas/median_pixel/configs/default.json\")\n",
    "#config[\"inherits\"] = \"models/image/configs/default\"\n",
    "config[\"input/data_spec\"] = specs_path\n",
    "config[\"input/annotation_name\"] = [\"polygon#annotator1\"]\n",
    "config[\"validation/checkpoint_metric_mode\"] = \"class_level\"\n",
    "\n",
    "\n",
    "\n",
    "# We want to merge the annotations from all annotators into one label mask\n",
    "config[\"input/merge_annotations\"] = \"union\"\n",
    "\n",
    "# We have a two-class problem and we want to ignore all unlabeled pixels\n",
    "# Everything which is >= settings.label_index_thresh will later be considered invalid\n",
    "config[\"label_mapping\"] = label_mapping_train  #leaving as none will use the label Id#s in the segmentation bloscs. if we want to remap the labels, we can specify here.\n",
    "                            #could be useful for combining multiple labels into one label, without reloading the intermediates?\n",
    "    #\"spleen\": 0,\n",
    "    #\"gallbladder\": 1,\n",
    "    #\"unlabeled\": settings.label_index_thresh,\n",
    "config[\"input/n_classes\"] = 21 #right now this needs to be set, or else it assumes 0\n",
    "#some confusion on how background is handled/defined\n",
    "\n",
    "#specify batch and sampler settings:\n",
    "config['dataloader_kwargs/batch_size'] = batch_size\n",
    "config['dataloader_kwargs/num_workers'] = num_workers\n",
    "\n",
    "# Reduce the training time\n",
    "config[\"trainer_kwargs/max_epochs\"] = 1\n",
    "\n",
    "# Progress bars can cause problems in Jupyter notebooks so we disable them here (training does not take super long)\n",
    "config[\"trainer_kwargs/enable_progress_bar\"] = True\n",
    "\n",
    "# Uncomment the following lines if you want to use one of the pretrained models as basis for our training\n",
    "# config[\"model/pretrained_model\"] = {\n",
    "#     \"model\": \"image\",\n",
    "#     \"run_folder\": \"2022-02-03_22-58-44_generated_default_model_comparison\",\n",
    "#\n",
    "\n",
    "config_path = external/'data'/ (name + \"_config.json\")\n",
    "config.save_config(config_path)\n",
    "JSON(config_path)\n",
    "\n",
    "print(type(config_path))\n",
    "print(label_mapping_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start the Training\n",
    "You are now ready to train your network. Simply run the `htc training` command and pass the model type (image model in our case) and path to the config as arguments.\n",
    "> &#x26a0;&#xfe0f; Starting a training session in a Jupyter notebook is usually not a good idea. Instead, it is advisable to use a [`screen`](https://linuxize.com/post/how-to-use-linux-screen/) environment so that your training runs in the background and you can return later to check for the status.\n",
    "\n",
    "> There is also a `--fold FOLD_NAME` switch if you only want to train only one fold. This is useful for debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/l328r/htc/htc/models/run_training.py\", line 23, in <module>\n",
      "    from htc.models.common.HTCLightning import HTCLightning\n",
      "  File \"/home/l328r/htc/htc/models/common/HTCLightning.py\", line 15, in <module>\n",
      "    from htc.models.common.EvaluationMixin import EvaluationMixin\n",
      "  File \"/home/l328r/htc/htc/models/common/EvaluationMixin.py\", line 12, in <module>\n",
      "    from htc.evaluation.evaluate_images import evaluate_images\n",
      "  File \"/home/l328r/htc/htc/evaluation/evaluate_images.py\", line 10, in <module>\n",
      "    from monai.metrics import SurfaceDistanceMetric, compute_dice, compute_surface_dice\n",
      "  File \"/home/l328r/.conda/envs/htc-dev/lib/python3.11/site-packages/monai/__init__.py\", line 58, in <module>\n",
      "    load_submodules(sys.modules[__name__], False, exclude_pattern=excludes)\n",
      "  File \"/home/l328r/.conda/envs/htc-dev/lib/python3.11/site-packages/monai/utils/module.py\", line 211, in load_submodules\n",
      "    mod = import_module(name)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/l328r/.conda/envs/htc-dev/lib/python3.11/importlib/__init__.py\", line 126, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/l328r/.conda/envs/htc-dev/lib/python3.11/site-packages/monai/apps/__init__.py\", line 14, in <module>\n",
      "    from .datasets import CrossValidation, DecathlonDataset, MedNISTDataset, TciaDataset\n",
      "  File \"/home/l328r/.conda/envs/htc-dev/lib/python3.11/site-packages/monai/apps/datasets.py\", line 24, in <module>\n",
      "    from monai.apps.tcia import (\n",
      "  File \"/home/l328r/.conda/envs/htc-dev/lib/python3.11/site-packages/monai/apps/tcia/__init__.py\", line 15, in <module>\n",
      "    from .utils import (\n",
      "  File \"/home/l328r/.conda/envs/htc-dev/lib/python3.11/site-packages/monai/apps/tcia/utils.py\", line 21, in <module>\n",
      "    requests_get, has_requests = optional_import(\"requests\", name=\"get\")\n",
      "                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/l328r/.conda/envs/htc-dev/lib/python3.11/site-packages/monai/utils/module.py\", line 399, in optional_import\n",
      "    pkg = __import__(module)  # top level module\n",
      "          ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/l328r/.conda/envs/htc-dev/lib/python3.11/site-packages/requests/__init__.py\", line 164, in <module>\n",
      "    from .api import delete, get, head, options, patch, post, put, request\n",
      "  File \"/home/l328r/.conda/envs/htc-dev/lib/python3.11/site-packages/requests/api.py\", line 11, in <module>\n",
      "    from . import sessions\n",
      "  File \"/home/l328r/.conda/envs/htc-dev/lib/python3.11/site-packages/requests/sessions.py\", line 15, in <module>\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Training was not successful",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 11\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Retrieve GPU memory information\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m#allocated_memory, total_memory = torch.cuda.mem_get_info()\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m#print(f\"Allocated Memory: {allocated_memory / (1024 ** 2):.2f} MB\")\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m#print(f\"Total Memory: {total_memory / (1024 ** 2):.2f} MB\")\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m#print(torch.cuda.memory_allocated())\u001b[39;00m\n\u001b[1;32m     10\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39msystem(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhtc training --model image --config $config_path\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m _exit_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining was not successful\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# noqa: F821\u001b[39;00m\n",
      "\u001b[0;31mAssertionError\u001b[0m: Training was not successful"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "# Retrieve GPU memory information\n",
    "#allocated_memory, total_memory = torch.cuda.mem_get_info()\n",
    "#print(f\"Allocated Memory: {allocated_memory / (1024 ** 2):.2f} MB\")\n",
    "#print(f\"Total Memory: {total_memory / (1024 ** 2):.2f} MB\")\n",
    "\n",
    "#print(torch.cuda.memory_allocated())\n",
    "\n",
    "!htc training --model image --config $config_path\n",
    "assert _exit_code == 0, \"Training was not successful\"  # noqa: F821"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: nvidia-smi: command not found\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def print_gpu_info():\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "        print(f\"PyTorch CUDA Support: {torch.cuda.is_available()}\")\n",
    "        print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
    "\n",
    "        for i in range(torch.cuda.device_count()):\n",
    "            print(f\"\\nGPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "            print(f\"  Total Memory: {torch.cuda.get_device_properties(i).total_memory / 1024 ** 3:.2f} GB\")\n",
    "            print(f\"  Allocated Memory: {torch.cuda.memory_allocated(i) / 1024 ** 2:.2f} MB\")\n",
    "            print(f\"  Cached Memory: {torch.cuda.memory_reserved(i) / 1024 ** 2:.2f} MB\")\n",
    "            print(f\"  Current Memory Usage: {torch.cuda.memory_allocated(i) / 1024 ** 2:.2f} MB\")\n",
    "            print(f\"  Peak Memory Usage: {torch.cuda.max_memory_allocated(i) / 1024 ** 2:.2f} MB\")\n",
    "            print(f\"  Free Memory: {(torch.cuda.get_device_properties(i).total_memory - torch.cuda.memory_allocated(i)) / 1024 ** 2:.2f} MB\")\n",
    "\n",
    "print_gpu_info()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "htc-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
