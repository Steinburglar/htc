{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segmentation Training Walkthrough\n",
    "This notebook will walk a user through using the Atlas compatible htc for training their own segmentation model. There is another, similar notebook for training a classification model based on spectral analysis, titled \"TissueAtlasClassificationTraining.py\" If you have not yet, please read the Setup tutorial for important information.\n",
    "Start with necessary inputs and define path to your dataset_settings .json. The tutorial is written with a very small dataset (2 pigs) called \"HeiPorSpectral_mod\". Replace relevant directory paths / names with the names to your own dataset and json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/omics/groups/OE0645/internal/data/htcdata/medium_test/external/intermediates\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "from IPython.display import JSON\n",
    "from typing import TYPE_CHECKING, Any, Callable, Union, Self\n",
    "from htc import (\n",
    "    Config,\n",
    "    DataPath,\n",
    "    DataSpecification,\n",
    "    MetricAggregation,\n",
    "    SpecsGeneration,\n",
    "    create_class_scores_figure,\n",
    "    settings,\n",
    ")\n",
    "from htc.models.data.SpecsGenerationAtlas import SpecsGenerationAtlas\n",
    "\n",
    "intermediates_dir = settings.intermediates_dirs.external\n",
    "print(intermediates_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can specify important parameters for your training run, such as fold, train/test split, etc. replace the values in the following code block with the values of your choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TO DO:\n",
    "#filter by existence of .txt file next to hypergui within timestamp folder -- lets you know that its ok\n",
    "#add batch size\n",
    "#add epoch length\n",
    "#add batch randomization conditions\n",
    "\n",
    "filter_txt = lambda p: p.contains_txt()\n",
    "filters = [filter_txt] #list of callable filter functions, can be a variety of things\n",
    "annotation_name = 'annotator1' #name of annotators to be used\n",
    "test_ratio = 0 #ratio of images to be saved as test, i.e, not ued in any training. should be float between 0.0 and 1.0\n",
    "n_folds = 3 #number of folds to make in the training data. training data (not test data) will be randomly split into n_folds different groups\n",
    "#for each \"fold\", the network will train a model with one of the groups as validation and all the other groups as training data.  \n",
    "seed = None #optional parameter that interacts with the random grouping of the folding operation. For a different fold upon every function call, set = None.\n",
    "# for a consistent fold, set seed to a number of your choice, e.g. seed = 42\n",
    "name = \"testSegment\" #name of a json file created in the following code block, that gets stored in the parent directory of this notebook. name it something simple and descriptive\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/omics/groups/OE0645/internal/data/htcdata/medium_test/external\n",
      "['P160_OP124_2023_06_28_Experiment1', 'P162_OP126_2023_07_06_Experiment1', 'P163_OP127_2023_07_12_Experiment1']\n"
     ]
    }
   ],
   "source": [
    "tutorial_dir = Path().absolute()\n",
    "external = settings.external_dir.external['path_dataset']#need brackets to acess the path, because settings.external_dir.external is a dictionary cointaing info about the external_dir.\n",
    "#settings.external_dir is an object containing all the different external directories: in our case, there should always just be one with shortcut \"external\"\n",
    "print(external)\n",
    "specs_path = external/'data' / name\n",
    "SpecsGenerationAtlas(intermediates_dir,\n",
    "                filters = filters,\n",
    "                annotation_name = annotation_name,\n",
    "                test_ratio = test_ratio,\n",
    "                n_folds = n_folds,\n",
    "                seed = seed,\n",
    "                name = name,\n",
    "                ).generate_dataset(external / 'data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lightning Class\n",
    "\n",
    "Next step is to choose/build our lightning class. The Lightning class (as in Pytorch Lightning) performs many aspects of managing training, and can b customized by creating your own child class. most notably, the Lightning class allows you to specify your Loss function.\n",
    "\n",
    "For this walkthrough, we will use the htc default \"LightingImage\" class, which is their default class for training on full images (as opposed to patches, pixels, or superpixels). This calculates loss as a weighted average of Dice loss and Cross-Entropy loss. See the htc \"networkTraining\" tutorial for more info on the lightning class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config\n",
    "The last step before training is to create our configuration file. This file is also a json that contains important metadata, and it is used by the training process itself to configure training hyperparameters, like batch size and transformations. We will use the htc's Config ***class*** to write the config ***json***\n",
    "\n",
    "The following Code block will write the config json for you. By default, it will store the config.json file in the same directory as your dataset_settings json.\n",
    "\n",
    "The following code block is still set up for tutorial use, not production use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assign training hyperparameters\n",
    "max_epochs = 2 #this can be whatever you want\n",
    "batch_size = 2 #this is the number of SUBJECTS, rather than images, in each batch. The loader is designed to sort batches by subject\n",
    "#default batch size is 3 subjects\n",
    "shuffle = True #this tells the batch generator to retrieve random, different, batches on every epoch.\n",
    "#True causes it to be random, False will leave same batches across epochs. \n",
    "num_workers = \"auto\" #how many dataloading \"worker\" subprocesses to start. The optimal amount for fast loading is highly dependant on your system\n",
    "#you can experiment on low-epoch runs to see what num_workers maximizes your training speed. \n",
    "#left to implement: specialized sampling practices? such as guaranteeing even organ distribution across classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pathlib.PosixPath'>\n"
     ]
    }
   ],
   "source": [
    "config = Config.from_model_name(\"default\", \"image\")\n",
    "config[\"inherits\"] = \"models/image/configs/default\"\n",
    "config[\"input/data_spec\"] = specs_path\n",
    "config[\"input/annotation_name\"] = [\"polygon#annotator1\"]\n",
    "config[\"validation/checkpoint_metric_mode\"] = \"class_level\"\n",
    "\n",
    "\n",
    "\n",
    "# We want to merge the annotations from all annotators into one label mask\n",
    "config[\"input/merge_annotations\"] = \"union\"\n",
    "\n",
    "# We have a two-class problem and we want to ignore all unlabeled pixels\n",
    "# Everything which is >= settings.label_index_thresh will later be considered invalid\n",
    "config[\"label_mapping\"] = {\n",
    "        \"last_valid_label_index\": 1,\n",
    "        \"mapping_index_name\": {\n",
    "            \"0\": \"uro_conduit\",\n",
    "            \"1\": \"background\",\n",
    "            \"254\": \"overlap\" },\n",
    "        \"mapping_name_index\": {\n",
    "            \"overlap\": 254,\n",
    "            \"unlabeled\": 1,\n",
    "            \"uro_conduit\": 0 },\n",
    "        \"unknown_invalid\": False,\n",
    "        \"zero_is_invalid\": False}\n",
    "#leaving as none will use the label Id#s in the segmentation bloscs. if we want to remap the labels, we can specify here.\n",
    "#could be useful for combining multiple labels into one label, without reloading the intermediates?\n",
    "#some confusion on how background is handled/defined\n",
    "\n",
    "#specify batch and sampler settings:\n",
    "config['dataloader_kwargs/batch_size'] = batch_size\n",
    "config['dataloader_kwargs/num_workers'] = num_workers\n",
    "\n",
    "# Reduce the training time\n",
    "config[\"trainer_kwargs/max_epochs\"] = max_epochs\n",
    "\n",
    "# Progress bars can cause problems in Jupyter notebooks so we disable them here (training does not take super long)\n",
    "config[\"trainer_kwargs/enable_progress_bar\"] = True\n",
    "\n",
    "# Uncomment the following lines if you want to use one of the pretrained models as basis for our training\n",
    "# config[\"model/pretrained_model\"] = {\n",
    "#     \"model\": \"image\",\n",
    "#     \"run_folder\": \"2022-02-03_22-58-44_generated_default_model_comparison\",\n",
    "#\n",
    "\n",
    "config_path = external/'data'/ (name + \"_config.json\")\n",
    "config.save_config(config_path)\n",
    "JSON(config_path)\n",
    "\n",
    "print(type(config_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start the Training\n",
    "You are now ready to train your network. Simply run the `htc training` command and pass the model type (image model in our case) and path to the config as arguments.\n",
    "> &#x26a0;&#xfe0f; Starting a training session in a Jupyter notebook is usually not a good idea. Instead, it is advisable to use a [`screen`](https://linuxize.com/post/how-to-use-linux-screen/) environment so that your training runs in the background and you can return later to check for the status.\n",
    "\n",
    "> There is also a `--fold FOLD_NAME` switch if you only want to train only one fold. This is useful for debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m[\u001b[0m\u001b[32mINFO\u001b[0m\u001b[1m]\u001b[0m\u001b[1m[\u001b[0m\u001b[3mhtc\u001b[0m\u001b[1m]\u001b[0m Starting training of the fold fold_1 \u001b[1m[\u001b[0m\u001b[37m1\u001b[0m/\u001b[37m3\u001b[0m\u001b[1m]\u001b[0m       \u001b[2mrun_training.py:301\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[32mINFO\u001b[0m\u001b[1m]\u001b[0m\u001b[1m[\u001b[0m\u001b[3mhtc\u001b[0m\u001b[1m]\u001b[0m The number of workers are set to \u001b[37m1\u001b[0m \u001b[1m(\u001b[0m\u001b[37m2\u001b[0m physical cores    \u001b[2mutils.py:250\u001b[0m\n",
      "are available in total\u001b[1m)\u001b[0m                                             \u001b[2m            \u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[32mINFO\u001b[0m\u001b[1m]\u001b[0m\u001b[1m[\u001b[0m\u001b[3mhtc\u001b[0m\u001b[1m]\u001b[0m The following config will be used for training:   \u001b[2mrun_training.py:81\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[32mINFO\u001b[0m\u001b[1m]\u001b[0m\u001b[1m[\u001b[0m\u001b[3mhtc\u001b[0m\u001b[1m]\u001b[0m \u001b[1m{\u001b[0m\u001b[90m'config_name'\u001b[0m: \u001b[90m'testSegment_config'\u001b[0m,             \u001b[2mrun_training.py:82\u001b[0m\n",
      " \u001b[90m'dataloader_kwargs'\u001b[0m: \u001b[1m{\u001b[0m\u001b[90m'batch_size'\u001b[0m: \u001b[37m2\u001b[0m, \u001b[90m'num_workers'\u001b[0m: \u001b[37m1\u001b[0m\u001b[1m}\u001b[0m,    \u001b[2m                  \u001b[0m\n",
      " \u001b[90m'input'\u001b[0m: \u001b[1m{\u001b[0m\u001b[90m'annotation_name'\u001b[0m: \u001b[1m[\u001b[0m\u001b[90m'\u001b[0m\u001b[36mpolygon#annotator1\u001b[0m\u001b[90m'\u001b[0m\u001b[1m]\u001b[0m,         \u001b[2m                  \u001b[0m\n",
      "           \u001b[90m'data_spec'\u001b[0m:                                       \u001b[2m                  \u001b[0m\n",
      "\u001b[90m'/omics/groups/OE0645/internal/data/htcdata/medium_test/exter\u001b[0m \u001b[2m                  \u001b[0m\n",
      "\u001b[90mnal/data/testSegment.json'\u001b[0m,                                   \u001b[2m                  \u001b[0m\n",
      "           \u001b[90m'epoch_size'\u001b[0m: \u001b[37m500\u001b[0m,                                 \u001b[2m                  \u001b[0m\n",
      "           \u001b[90m'merge_annotations'\u001b[0m: \u001b[90m'union'\u001b[0m,                      \u001b[2m                  \u001b[0m\n",
      "           \u001b[90m'n_channels'\u001b[0m: \u001b[37m100\u001b[0m,                                 \u001b[2m                  \u001b[0m\n",
      "           \u001b[90m'preprocessing'\u001b[0m: \u001b[90m'L1'\u001b[0m,                             \u001b[2m                  \u001b[0m\n",
      "           \u001b[90m'transforms_gpu'\u001b[0m: \u001b[1m[\u001b[0m\u001b[1m{\u001b[0m\u001b[90m'class'\u001b[0m: \u001b[90m'KorniaTransform'\u001b[0m,    \u001b[2m                  \u001b[0m\n",
      "                               \u001b[90m'degrees'\u001b[0m: \u001b[37m45\u001b[0m,                 \u001b[2m                  \u001b[0m\n",
      "                               \u001b[90m'p'\u001b[0m: \u001b[37m0.5\u001b[0m,                      \u001b[2m                  \u001b[0m\n",
      "                               \u001b[90m'padding_mode'\u001b[0m: \u001b[90m'reflection'\u001b[0m,  \u001b[2m                  \u001b[0m\n",
      "                               \u001b[90m'scale'\u001b[0m: \u001b[1m[\u001b[0m\u001b[37m0.9\u001b[0m, \u001b[37m1.1\u001b[0m\u001b[1m]\u001b[0m,           \u001b[2m                  \u001b[0m\n",
      "                               \u001b[90m'transformation_name'\u001b[0m:         \u001b[2m                  \u001b[0m\n",
      "\u001b[90m'RandomAffine'\u001b[0m,                                               \u001b[2m                  \u001b[0m\n",
      "                               \u001b[90m'translate'\u001b[0m: \u001b[1m[\u001b[0m\u001b[37m0.0625\u001b[0m,          \u001b[2m                  \u001b[0m\n",
      "\u001b[37m0.0625\u001b[0m\u001b[1m]\u001b[0m\u001b[1m}\u001b[0m,                                                     \u001b[2m                  \u001b[0m\n",
      "                              \u001b[1m{\u001b[0m\u001b[90m'class'\u001b[0m: \u001b[90m'KorniaTransform'\u001b[0m,    \u001b[2m                  \u001b[0m\n",
      "                               \u001b[90m'p'\u001b[0m: \u001b[37m0.25\u001b[0m,                     \u001b[2m                  \u001b[0m\n",
      "                               \u001b[90m'transformation_name'\u001b[0m:         \u001b[2m                  \u001b[0m\n",
      "\u001b[90m'RandomHorizontalFlip'\u001b[0m\u001b[1m}\u001b[0m,                                      \u001b[2m                  \u001b[0m\n",
      "                              \u001b[1m{\u001b[0m\u001b[90m'class'\u001b[0m: \u001b[90m'KorniaTransform'\u001b[0m,    \u001b[2m                  \u001b[0m\n",
      "                               \u001b[90m'p'\u001b[0m: \u001b[37m0.25\u001b[0m,                     \u001b[2m                  \u001b[0m\n",
      "                               \u001b[90m'transformation_name'\u001b[0m:         \u001b[2m                  \u001b[0m\n",
      "\u001b[90m'RandomVerticalFlip'\u001b[0m\u001b[1m}\u001b[0m\u001b[1m]\u001b[0m\u001b[1m}\u001b[0m,                                      \u001b[2m                  \u001b[0m\n",
      " \u001b[90m'label_mapping'\u001b[0m: \u001b[1m{\u001b[0m\u001b[90m'last_valid_label_index'\u001b[0m: \u001b[37m1\u001b[0m,               \u001b[2m                  \u001b[0m\n",
      "                   \u001b[90m'mapping_index_name'\u001b[0m: \u001b[1m{\u001b[0m\u001b[90m'0'\u001b[0m: \u001b[90m'uro_conduit'\u001b[0m, \u001b[2m                  \u001b[0m\n",
      "                                          \u001b[90m'1'\u001b[0m: \u001b[90m'background'\u001b[0m,  \u001b[2m                  \u001b[0m\n",
      "                                          \u001b[90m'254'\u001b[0m: \u001b[90m'overlap'\u001b[0m\u001b[1m}\u001b[0m,  \u001b[2m                  \u001b[0m\n",
      "                   \u001b[90m'mapping_name_index'\u001b[0m: \u001b[1m{\u001b[0m\u001b[90m'overlap'\u001b[0m: \u001b[37m254\u001b[0m,     \u001b[2m                  \u001b[0m\n",
      "                                          \u001b[90m'unlabeled'\u001b[0m: \u001b[37m1\u001b[0m,     \u001b[2m                  \u001b[0m\n",
      "                                          \u001b[90m'uro_conduit'\u001b[0m: \u001b[37m0\u001b[0m\u001b[1m}\u001b[0m,  \u001b[2m                  \u001b[0m\n",
      "                   \u001b[90m'unknown_invalid'\u001b[0m: \u001b[3;91mFalse\u001b[0m,                  \u001b[2m                  \u001b[0m\n",
      "                   \u001b[90m'zero_is_invalid'\u001b[0m: \u001b[3;91mFalse\u001b[0m\u001b[1m}\u001b[0m,                 \u001b[2m                  \u001b[0m\n",
      " \u001b[90m'lightning_class'\u001b[0m:                                           \u001b[2m                  \u001b[0m\n",
      "\u001b[90m'htc.models.image.LightningImage>LightningImage'\u001b[0m,             \u001b[2m                  \u001b[0m\n",
      " \u001b[90m'model'\u001b[0m: \u001b[1m{\u001b[0m\u001b[90m'architecture_kwargs'\u001b[0m: \u001b[1m{\u001b[0m\u001b[90m'encoder_name'\u001b[0m:            \u001b[2m                  \u001b[0m\n",
      "\u001b[90m'efficientnet-b5'\u001b[0m,                                            \u001b[2m                  \u001b[0m\n",
      "                                   \u001b[90m'encoder_weights'\u001b[0m:         \u001b[2m                  \u001b[0m\n",
      "\u001b[90m'imagenet'\u001b[0m\u001b[1m}\u001b[0m,                                                  \u001b[2m                  \u001b[0m\n",
      "           \u001b[90m'architecture_name'\u001b[0m: \u001b[90m'Unet'\u001b[0m,                       \u001b[2m                  \u001b[0m\n",
      "           \u001b[90m'model_name'\u001b[0m: \u001b[90m'ModelImage'\u001b[0m\u001b[1m}\u001b[0m,                       \u001b[2m                  \u001b[0m\n",
      " \u001b[90m'optimization'\u001b[0m: \u001b[1m{\u001b[0m\u001b[90m'lr_scheduler'\u001b[0m: \u001b[1m{\u001b[0m\u001b[90m'gamma'\u001b[0m: \u001b[37m0.99\u001b[0m, \u001b[90m'name'\u001b[0m:     \u001b[2m                  \u001b[0m\n",
      "\u001b[90m'ExponentialLR'\u001b[0m\u001b[1m}\u001b[0m,                                             \u001b[2m                  \u001b[0m\n",
      "                  \u001b[90m'optimizer'\u001b[0m: \u001b[1m{\u001b[0m\u001b[90m'lr'\u001b[0m: \u001b[37m0.001\u001b[0m,                  \u001b[2m                  \u001b[0m\n",
      "                                \u001b[90m'name'\u001b[0m: \u001b[90m'Adam'\u001b[0m,               \u001b[2m                  \u001b[0m\n",
      "                                \u001b[90m'weight_decay'\u001b[0m: \u001b[37m0\u001b[0m\u001b[1m}\u001b[0m\u001b[1m}\u001b[0m,          \u001b[2m                  \u001b[0m\n",
      " \u001b[90m'swa_kwargs'\u001b[0m: \u001b[1m{\u001b[0m\u001b[90m'annealing_epochs'\u001b[0m: \u001b[37m0\u001b[0m\u001b[1m}\u001b[0m,                       \u001b[2m                  \u001b[0m\n",
      " \u001b[90m'trainer_kwargs'\u001b[0m: \u001b[1m{\u001b[0m\u001b[90m'accelerator'\u001b[0m: \u001b[90m'gpu'\u001b[0m,                     \u001b[2m                  \u001b[0m\n",
      "                    \u001b[90m'devices'\u001b[0m: \u001b[37m1\u001b[0m,                             \u001b[2m                  \u001b[0m\n",
      "                    \u001b[90m'enable_progress_bar'\u001b[0m: \u001b[3;92mTrue\u001b[0m,              \u001b[2m                  \u001b[0m\n",
      "                    \u001b[90m'max_epochs'\u001b[0m: \u001b[37m2\u001b[0m,                          \u001b[2m                  \u001b[0m\n",
      "                    \u001b[90m'precision'\u001b[0m: \u001b[90m'16-mixed'\u001b[0m\u001b[1m}\u001b[0m,                 \u001b[2m                  \u001b[0m\n",
      " \u001b[90m'validation'\u001b[0m: \u001b[1m{\u001b[0m\u001b[90m'checkpoint_metric'\u001b[0m: \u001b[90m'dice_metric'\u001b[0m,           \u001b[2m                  \u001b[0m\n",
      "                \u001b[90m'checkpoint_metric_mode'\u001b[0m: \u001b[90m'class_level'\u001b[0m,      \u001b[2m                  \u001b[0m\n",
      "                \u001b[90m'dataset_index'\u001b[0m: \u001b[37m0\u001b[0m\u001b[1m}\u001b[0m\u001b[1m}\u001b[0m                          \u001b[2m                  \u001b[0m\n",
      "Seed set to 1337\n",
      "\u001b[1m[\u001b[0m\u001b[94mDEBUG\u001b[0m\u001b[1m]\u001b[0m\u001b[1m[\u001b[0m\u001b[3mhtc\u001b[0m\u001b[1m]\u001b[0m Used transformations:                             \u001b[2mtransforms.py:124\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[1;35mToType\u001b[0m\u001b[1m(\u001b[0m\u001b[33mdtype\u001b[0m=\u001b[35mtorch\u001b[0m.float16\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m                                  \u001b[2m                 \u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[94mDEBUG\u001b[0m\u001b[1m]\u001b[0m\u001b[1m[\u001b[0m\u001b[3mhtc\u001b[0m\u001b[1m]\u001b[0m Used transformations:                             \u001b[2mtransforms.py:124\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[1;35mToType\u001b[0m\u001b[1m(\u001b[0m\u001b[33mdtype\u001b[0m=\u001b[35mtorch\u001b[0m.float16\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m                                  \u001b[2m                 \u001b[0m\n",
      "constructed lightning class\n",
      "\u001b[1m[\u001b[0m\u001b[1;7;31mCRITICAL\u001b[0m\u001b[1m]\u001b[0m\u001b[1m[\u001b[0m\u001b[3mhtc\u001b[0m\u001b[1m]\u001b[0m Uncaught exception:                          \u001b[2mrun_training.py:384\u001b[0m\n",
      "Traceback \u001b[1m(\u001b[0mmost recent call last\u001b[1m)\u001b[0m:                           \u001b[2m                   \u001b[0m\n",
      "  File \u001b[90m\"/home/l328r/htc/htc/models/run_training.py\"\u001b[0m, line    \u001b[2m                   \u001b[0m\n",
      "\u001b[37m396\u001b[0m, in \u001b[1m<\u001b[0m\u001b[1;95mmodule\u001b[0m\u001b[39m>\u001b[0m                                             \u001b[2m                   \u001b[0m\n",
      "\u001b[39m    \u001b[0m\u001b[1;35mfold_trainer.train_fold\u001b[0m\u001b[1;39m(\u001b[0m\u001b[39margs.run_folder, args.fold_name,\u001b[0m \u001b[2m                   \u001b[0m\n",
      "\u001b[39margs.test, file_log_handler\u001b[0m\u001b[1;39m)\u001b[0m                                 \u001b[2m                   \u001b[0m\n",
      "\u001b[39m  File \u001b[0m\u001b[90m\"/home/l328r/htc/htc/models/run_training.py\"\u001b[0m\u001b[39m, line \u001b[0m   \u001b[2m                   \u001b[0m\n",
      "\u001b[37m67\u001b[0m\u001b[39m, in train_fold\u001b[0m                                            \u001b[2m                   \u001b[0m\n",
      "\u001b[39m    \u001b[0m\u001b[1;35mself._train_fold\u001b[0m\u001b[1;39m(\u001b[0m\u001b[39mmodel_dir_tmp, fold_name, *args\u001b[0m\u001b[1;39m)\u001b[0m        \u001b[2m                   \u001b[0m\n",
      "\u001b[39m  File \u001b[0m\u001b[90m\"/home/l328r/htc/htc/models/run_training.py\"\u001b[0m\u001b[39m, line \u001b[0m   \u001b[2m                   \u001b[0m\n",
      "\u001b[37m165\u001b[0m\u001b[39m, in _train_fold\u001b[0m                                          \u001b[2m                   \u001b[0m\n",
      "\u001b[39m    module = \u001b[0m\u001b[1;35mself.LightningClass\u001b[0m\u001b[1;39m(\u001b[0m\u001b[39mdataset_train, \u001b[0m             \u001b[2m                   \u001b[0m\n",
      "\u001b[39mdatasets_val, self.config, \u001b[0m\u001b[33mfold_name\u001b[0m\u001b[39m=\u001b[0m\u001b[35mfold_name\u001b[0m\u001b[39m, \u001b[0m             \u001b[2m                   \u001b[0m\n",
      "\u001b[39m**lightning_kwargs\u001b[0m\u001b[1;39m)\u001b[0m                                          \u001b[2m                   \u001b[0m\n",
      "\u001b[39m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m \u001b[2m                   \u001b[0m\n",
      "\u001b[39m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m      \u001b[2m                   \u001b[0m\n",
      "\u001b[39m  File \u001b[0m\u001b[90m\"/home/l328r/htc/htc/models/image/LightningImage.py\"\u001b[0m\u001b[39m,\u001b[0m \u001b[2m                   \u001b[0m\n",
      "\u001b[39mline \u001b[0m\u001b[37m40\u001b[0m\u001b[39m, in __init__\u001b[0m                                         \u001b[2m                   \u001b[0m\n",
      "\u001b[39m    self.model = \u001b[0m\u001b[1;35mModelClass\u001b[0m\u001b[1;39m(\u001b[0m\u001b[39mself.config\u001b[0m\u001b[1;39m)\u001b[0m                     \u001b[2m                   \u001b[0m\n",
      "\u001b[39m                 ^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m                     \u001b[2m                   \u001b[0m\n",
      "\u001b[39m  File \u001b[0m\u001b[90m\"/home/l328r/htc/htc/models/common/HTCModel.py\"\u001b[0m\u001b[39m, line\u001b[0m \u001b[2m                   \u001b[0m\n",
      "\u001b[37m25\u001b[0m\u001b[39m, in __call__\u001b[0m                                              \u001b[2m                   \u001b[0m\n",
      "\u001b[39m    \u001b[0m\u001b[1;35mobj.__post__init__\u001b[0m\u001b[1;39m(\u001b[0m\u001b[1;39m)\u001b[0m                                     \u001b[2m                   \u001b[0m\n",
      "\u001b[39m  File \u001b[0m\u001b[90m\"/home/l328r/htc/htc/models/common/HTCModel.py\"\u001b[0m\u001b[39m, line\u001b[0m \u001b[2m                   \u001b[0m\n",
      "\u001b[37m141\u001b[0m\u001b[39m, in __post__init__\u001b[0m                                       \u001b[2m                   \u001b[0m\n",
      "\u001b[39m    if self.config\u001b[0m\u001b[1;39m[\u001b[0m\u001b[90m\"model/pretrained_model\"\u001b[0m\u001b[1;39m]\u001b[0m\u001b[39m:\u001b[0m                \u001b[2m                   \u001b[0m\n",
      "\u001b[39m       ~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m                 \u001b[2m                   \u001b[0m\n",
      "\u001b[39m  File \u001b[0m\u001b[90m\"/home/l328r/htc/htc/utils/Config.py\"\u001b[0m\u001b[39m, line \u001b[0m\u001b[37m27\u001b[0m\u001b[39m, in \u001b[0m   \u001b[2m                   \u001b[0m\n",
      "\u001b[39m_track_key_usage\u001b[0m                                             \u001b[2m                   \u001b[0m\n",
      "\u001b[39m    self._used_keys\u001b[0m\u001b[39m = \u001b[0m\u001b[37m1\u001b[0m\u001b[39m  # We re-use a dict, but we are only\u001b[0m \u001b[2m                   \u001b[0m\n",
      "\u001b[39minterested in the unique set of keys\u001b[0m                         \u001b[2m                   \u001b[0m\n",
      "\u001b[39m    ~~~~~~~~~~~~~~~^^^^^^^^^^^^\u001b[0m                              \u001b[2m                   \u001b[0m\n",
      "\u001b[39m  File \u001b[0m\u001b[90m\"<string\u001b[0m\u001b[1;90m>\u001b[0m\u001b[90m\"\u001b[0m, line \u001b[37m2\u001b[0m, in __setitem__                    \u001b[2m                   \u001b[0m\n",
      "  File                                                       \u001b[2m                   \u001b[0m\n",
      "\u001b[90m\"/home/l328r/.conda/envs/htc-dev/lib/python3.11/multiprocess\u001b[0m \u001b[2m                   \u001b[0m\n",
      "\u001b[90ming/managers.py\"\u001b[0m, line \u001b[37m821\u001b[0m, in _callmethod                   \u001b[2m                   \u001b[0m\n",
      "    \u001b[1;35mconn.send\u001b[0m\u001b[1m(\u001b[0m\u001b[1m(\u001b[0mself._id, methodname, args, kwds\u001b[1m)\u001b[0m\u001b[1m)\u001b[0m            \u001b[2m                   \u001b[0m\n",
      "  File                                                       \u001b[2m                   \u001b[0m\n",
      "\u001b[90m\"/home/l328r/.conda/envs/htc-dev/lib/python3.11/multiprocess\u001b[0m \u001b[2m                   \u001b[0m\n",
      "\u001b[90ming/connection.py\"\u001b[0m, line \u001b[37m206\u001b[0m, in send                        \u001b[2m                   \u001b[0m\n",
      "    \u001b[1;35mself._send_bytes\u001b[0m\u001b[1m(\u001b[0m\u001b[1;35m_ForkingPickler.dumps\u001b[0m\u001b[1m(\u001b[0mobj\u001b[1m)\u001b[0m\u001b[1m)\u001b[0m             \u001b[2m                   \u001b[0m\n",
      "  File                                                       \u001b[2m                   \u001b[0m\n",
      "\u001b[90m\"/home/l328r/.conda/envs/htc-dev/lib/python3.11/multiprocess\u001b[0m \u001b[2m                   \u001b[0m\n",
      "\u001b[90ming/connection.py\"\u001b[0m, line \u001b[37m427\u001b[0m, in _send_bytes                 \u001b[2m                   \u001b[0m\n",
      "    \u001b[1;35mself._send\u001b[0m\u001b[1m(\u001b[0mheader + buf\u001b[1m)\u001b[0m                                 \u001b[2m                   \u001b[0m\n",
      "  File                                                       \u001b[2m                   \u001b[0m\n",
      "\u001b[90m\"/home/l328r/.conda/envs/htc-dev/lib/python3.11/multiprocess\u001b[0m \u001b[2m                   \u001b[0m\n",
      "\u001b[90ming/connection.py\"\u001b[0m, line \u001b[37m384\u001b[0m, in _send                       \u001b[2m                   \u001b[0m\n",
      "    n = \u001b[1;35mwrite\u001b[0m\u001b[1m(\u001b[0mself._handle, buf\u001b[1m)\u001b[0m                             \u001b[2m                   \u001b[0m\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^                             \u001b[2m                   \u001b[0m\n",
      "BrokenPipeError: \u001b[1m[\u001b[0mErrno \u001b[37m32\u001b[0m\u001b[1m]\u001b[0m Broken pipe                      \u001b[2m                   \u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/l328r/htc/htc/models/run_training.py\", line 396, in <module>\n",
      "    fold_trainer.train_fold(args.run_folder, args.fold_name, args.test, file_log_handler)\n",
      "  File \"/home/l328r/htc/htc/models/run_training.py\", line 67, in train_fold\n",
      "    self._train_fold(model_dir_tmp, fold_name, *args)\n",
      "  File \"/home/l328r/htc/htc/models/run_training.py\", line 165, in _train_fold\n",
      "    module = self.LightningClass(dataset_train, datasets_val, self.config, fold_name=fold_name, **lightning_kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/l328r/htc/htc/models/image/LightningImage.py\", line 40, in __init__\n",
      "    self.model = ModelClass(self.config)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/l328r/htc/htc/models/common/HTCModel.py\", line 25, in __call__\n",
      "    obj.__post__init__()\n",
      "  File \"/home/l328r/htc/htc/models/common/HTCModel.py\", line 141, in __post__init__\n",
      "    if self.config[\"model/pretrained_model\"]:\n",
      "       ~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/l328r/htc/htc/utils/Config.py\", line 27, in _track_key_usage\n",
      "    self._used_keys[identifier] = 1  # We re-use a dict, but we are only interested in the unique set of keys\n",
      "    ~~~~~~~~~~~~~~~^^^^^^^^^^^^\n",
      "  File \"<string>\", line 2, in __setitem__\n",
      "  File \"/home/l328r/.conda/envs/htc-dev/lib/python3.11/multiprocessing/managers.py\", line 821, in _callmethod\n",
      "    conn.send((self._id, methodname, args, kwds))\n",
      "  File \"/home/l328r/.conda/envs/htc-dev/lib/python3.11/multiprocessing/connection.py\", line 206, in send\n",
      "    self._send_bytes(_ForkingPickler.dumps(obj))\n",
      "  File \"/home/l328r/.conda/envs/htc-dev/lib/python3.11/multiprocessing/connection.py\", line 427, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/home/l328r/.conda/envs/htc-dev/lib/python3.11/multiprocessing/connection.py\", line 384, in _send\n",
      "    n = write(self._handle, buf)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "^C\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Training was not successful",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 11\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Retrieve GPU memory information\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m#allocated_memory, total_memory = torch.cuda.mem_get_info()\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m#print(f\"Allocated Memory: {allocated_memory / (1024 ** 2):.2f} MB\")\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m#print(f\"Total Memory: {total_memory / (1024 ** 2):.2f} MB\")\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m#print(torch.cuda.memory_allocated())\u001b[39;00m\n\u001b[1;32m     10\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39msystem(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhtc training --model image --config $config_path\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m _exit_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining was not successful\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# noqa: F821\u001b[39;00m\n",
      "\u001b[0;31mAssertionError\u001b[0m: Training was not successful"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "# Retrieve GPU memory information\n",
    "#allocated_memory, total_memory = torch.cuda.mem_get_info()\n",
    "#print(f\"Allocated Memory: {allocated_memory / (1024 ** 2):.2f} MB\")\n",
    "#print(f\"Total Memory: {total_memory / (1024 ** 2):.2f} MB\")\n",
    "\n",
    "#print(torch.cuda.memory_allocated())\n",
    "\n",
    "!htc training --model image --config $config_path\n",
    "assert _exit_code == 0, \"Training was not successful\"  # noqa: F821"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: nvidia-smi: command not found\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def print_gpu_info():\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "        print(f\"PyTorch CUDA Support: {torch.cuda.is_available()}\")\n",
    "        print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
    "\n",
    "        for i in range(torch.cuda.device_count()):\n",
    "            print(f\"\\nGPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "            print(f\"  Total Memory: {torch.cuda.get_device_properties(i).total_memory / 1024 ** 3:.2f} GB\")\n",
    "            print(f\"  Allocated Memory: {torch.cuda.memory_allocated(i) / 1024 ** 2:.2f} MB\")\n",
    "            print(f\"  Cached Memory: {torch.cuda.memory_reserved(i) / 1024 ** 2:.2f} MB\")\n",
    "            print(f\"  Current Memory Usage: {torch.cuda.memory_allocated(i) / 1024 ** 2:.2f} MB\")\n",
    "            print(f\"  Peak Memory Usage: {torch.cuda.max_memory_allocated(i) / 1024 ** 2:.2f} MB\")\n",
    "            print(f\"  Free Memory: {(torch.cuda.get_device_properties(i).total_memory - torch.cuda.memory_allocated(i)) / 1024 ** 2:.2f} MB\")\n",
    "\n",
    "print_gpu_info()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "htc-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
