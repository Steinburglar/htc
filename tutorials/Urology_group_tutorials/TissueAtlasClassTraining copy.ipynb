{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tissue Atlas Classification Training Walkthrough\n",
    "This notebook will walk a user through using the Atlas compatible htc for training their own organ classification (not segmentation) model based on spectral analysis. There is another, similar notebook for training a segmentation model, named \"SegmentationTraining.ipynb\" If you have not yet, please read the Setup tutorial for important information.\n",
    "Start with necessary inputs and define path to your dataset_settings .json. The tutorial is written with a very small dataset (2 pigs) called \"HeiPorSpectral_mod\". Replace relevant directory paths / names with the names to your own dataset and json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/l328r/htc/tutorials/Urology_group_tutorials\n",
      "/home/l328r/htc\n",
      "Current working directory: /home/l328r/htc\n"
     ]
    }
   ],
   "source": [
    "# This is a Python cell to define the relative navigation steps\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Get the current notebook directory\n",
    "notebook_dir = Path().resolve()\n",
    "print(notebook_dir)\n",
    "\n",
    "# Define the relative path to the root directory (e.g., go up 2 levels)\n",
    "levels_up = 2  # Adjust this based on your project structure\n",
    "repo_root = notebook_dir.parents[levels_up - 1]  # Adjust the index based on the levels\n",
    "\n",
    "print(repo_root)\n",
    "# Save the root directory path to an environment variable\n",
    "os.chdir(repo_root)\n",
    "\n",
    "# Verify the current working directory\n",
    "current_dir = Path().resolve()\n",
    "print(f\"Current working directory: {current_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/l328r/htc\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "/omics/groups/OE0645/internal/data/htcdata/medium_test/external/intermediates\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from htc.tissue_atlas.settings_atlas import SettingsAtlas\n",
    "import pandas as pd\n",
    "from IPython.display import JSON\n",
    "from typing import TYPE_CHECKING, Any, Callable, Union, Self\n",
    "from htc import (\n",
    "    Config,\n",
    "    DataPath,\n",
    "    DataSpecification,\n",
    "    MetricAggregation,\n",
    "    SpecsGeneration,\n",
    "    create_class_scores_figure,\n",
    "    settings,\n",
    ")\n",
    "from htc.models.data.SpecsGenerationAtlas import SpecsGenerationAtlas\n",
    "\n",
    "intermediates_dir = settings.intermediates_dirs.external\n",
    "print(intermediates_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can specify important parameters for your training run, such as fold, train/test split, etc. replace the values in the following code block with the values of your choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TO DO:\n",
    "#filter by existence of .txt file next to hypergui within timestamp folder -- lets you know that its ok\n",
    "#add batch size\n",
    "#add epoch length\n",
    "#add batch randomization conditions\n",
    "\n",
    "filter_txt = lambda p: p.contains_txt()\n",
    "filters = [filter_txt] #list of callable filter functions, can be a variety of things\n",
    "annotation_name = 'annotator1' #name of annotators to be used\n",
    "test_ratio = 0.33 #ratio of subjects to be saved as test, i.e, not ued in any training. should be float between 0.0 and 1.0 (if 1/3, write as 0.33)\n",
    "n_folds = 2 #number of folds to make in the training data. training data (not test data) will be randomly split into n_folds different groups\n",
    "#for each \"fold\", the network will train a model with one of the groups as validation and all the other groups as training data.  \n",
    "seed = None #optional parameter that interacts with the random grouping of the folding operation. For a different fold upon every function call, set = None.\n",
    "# for a consistent fold, set seed to a number of your choice, e.g. seed = 42\n",
    "name = \"Atlas\" #name of a json file created in the following code block, that gets stored in the parent directory of this notebook. name it something simple and descriptive\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/omics/groups/OE0645/internal/data/htcdata/medium_test/external\n",
      "['P160_OP124_2023_06_28_Experiment1', 'P162_OP126_2023_07_06_Experiment1', 'P163_OP127_2023_07_12_Experiment1']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span><span style=\"color: #808000; text-decoration-color: #808000\">WARNING</span><span style=\"font-weight: bold\">][</span><span style=\"font-style: italic\">htc</span><span style=\"font-weight: bold\">]</span> The environment variable PATH_Tivita_ureter_annotations was set to                        <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">Datasets.py:125</span>\n",
       "<span style=\"color: #800080; text-decoration-color: #800080\">/home/l328r/htc/</span>\"<span style=\"color: #800080; text-decoration-color: #800080\">/omics/groups/OE0645/internal/data/htcdata/ureter/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">mitk_annotations</span> but the path does    <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "not exist                                                                                                <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\u001b[33mWARNING\u001b[0m\u001b[1m]\u001b[0m\u001b[1m[\u001b[0m\u001b[3mhtc\u001b[0m\u001b[1m]\u001b[0m The environment variable PATH_Tivita_ureter_annotations was set to                        \u001b[2mDatasets.py:125\u001b[0m\n",
       "\u001b[35m/home/l328r/htc/\u001b[0m\"\u001b[35m/omics/groups/OE0645/internal/data/htcdata/ureter/\u001b[0m\u001b[95mmitk_annotations\u001b[0m but the path does    \u001b[2m               \u001b[0m\n",
       "not exist                                                                                                \u001b[2m               \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tutorial_dir = Path().absolute()\n",
    "external = settings.external_dir.external['path_dataset']#need brackets to acess the path, because settings.external_dir.external is a dictionary cointaing info about the external_dir.\n",
    "#settings.external_dir is an object containing all the different external directories: in our case, there should always just be one with shortcut \"external\"\n",
    "print(external)\n",
    "specs_path = external/'data' / name\n",
    "SpecsGenerationAtlas(intermediates_dir,\n",
    "                filters = filters,\n",
    "                annotation_name = annotation_name,\n",
    "                test_ratio = test_ratio,\n",
    "                n_folds = n_folds,\n",
    "                seed = seed,\n",
    "                name = name,\n",
    "                ).generate_dataset(external / 'data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lightning Class\n",
    "\n",
    "Next step is to choose/build our lightning class. The Lightning class (as in Pytorch Lightning) performs many aspects of managing training, and can b customized by creating your own child class. most notably, the Lightning class allows you to specify your Loss function.\n",
    "\n",
    "For this walkthrough, we will use the htc default \"LightingImage\" class, which is their default class for training on full images (as opposed to patches, pixels, or superpixels). This calculates loss as a weighted average of Dice loss and Cross-Entropy loss. See the htc \"networkTraining\" tutorial for more info on the lightning class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config\n",
    "The last step before training is to create our configuration file. This file is also a json that contains important metadata, and it is used by the training process itself to configure training hyperparameters, like batch size and transformations. We will use the htc's Config ***class*** to write the config ***json***\n",
    "\n",
    "The following Code block will write the config json for you. By default, it will store the config.json file in the same directory as your dataset_settings json."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assign training hyperparameters\n",
    "max_epochs = 1 #this can be whatever you want\n",
    "batch_size = 20000 #batch size works differently for the median Ã¼pixel model. here, it shoould be quite large.\n",
    "#CANNOT BE BATCH SIZE 1, breaks the batch norm steps?\n",
    "#default batch size is 3 subjects\n",
    "shuffle = True #this tells the batch generator to retrieve random, different, batches on every epoch.\n",
    "#True causes it to be random, False will leave same batches across epochs. \n",
    "num_workers = \"auto\" #how many dataloading \"worker\" subprocesses to start. The optimal amount for fast loading is highly dependant on your system\n",
    "#you can experiment on low-epoch runs to see what num_workers maximizes your training speed. \n",
    "#left to implement: specialized sampling practices? such as guaranteeing even organ distribution across classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For training, we use a new label mapping to read and understand the annotations. there is an existing label mapping defined in the SettingsAtlas class, which we will use here for the tutorial.\n",
    "If you have different requirements for a settings atlas, you could either write the label mapping yourself, using the settings atlas as a guide, or modify the one in settings atlas.\n",
    "\n",
    "Notably, It is possible to map multiple labels in your annotations to the same value for training, so that the model thinks of them as the same. (obviously, it is not possible to map one label in annotations to multiple classes in training). If you do so, you should make sure you also define mapping_index_name, to be clear about what name you want to recover from that class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load up settingsAtlas object, which contains among other properties a label mapping, that can be used by the training\n",
    "Settings_Atlas = SettingsAtlas()\n",
    "label_mapping_train = Settings_Atlas.label_mapping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/omics/groups/OE0645/internal/data/htcdata/medium_test/external/data/Atlas_config.json\n",
      "LabelMapping(stomach=0, small_bowel=1, colon=2, liver=3, gallbladder=4, pancreas=5, kidney=6, lung=7, heart=8, cartilage=9, bile_fluid=10, kidney_with_Gerotas_fascia=11, major_vein=12, peritoneum=13, muscle=14, skin=15, bone=16, omentum=17, bladder=18, spleen=19, uro_conduit=20)\n"
     ]
    }
   ],
   "source": [
    "config = Config(\"htc/tissue_atlas/median_pixel/configs/default.json\")\n",
    "#config[\"inherits\"] = \"/home/l328r/htc/htc/tissue_atlas/median_pixel/configs/default.json\"\n",
    "config[\"input/data_spec\"] = specs_path\n",
    "config[\"input/annotation_name\"] = [\"polygon#annotator1\"]\n",
    "config[\"validation/checkpoint_metric_mode\"] = \"class_level\"\n",
    "\n",
    "\n",
    "\n",
    "# We want to merge the annotations from all annotators into one label mask\n",
    "config[\"input/merge_annotations\"] = \"union\"\n",
    "\n",
    "# We have a two-class problem and we want to ignore all unlabeled pixels\n",
    "# Everything which is >= settings.label_index_thresh will later be considered invalid\n",
    "config[\"label_mapping\"] = label_mapping_train  #leaving as none will use the label Id#s in the segmentation bloscs. if we want to remap the labels, we can specify here.\n",
    "                            #could be useful for combining multiple labels into one label, without reloading the intermediates?\n",
    "    #\"spleen\": 0,\n",
    "    #\"gallbladder\": 1,\n",
    "    #\"unlabeled\": settings.label_index_thresh,\n",
    "config[\"input/n_classes\"] = 21 #right now this needs to be set, or else it assumes 0\n",
    "#some confusion on how background is handled/defined\n",
    "\n",
    "#specify batch and sampler settings:\n",
    "config['dataloader_kwargs/batch_size'] = batch_size\n",
    "config['dataloader_kwargs/num_workers'] = num_workers\n",
    "\n",
    "# Reduce the training time\n",
    "config[\"trainer_kwargs/max_epochs\"] = 1\n",
    "\n",
    "# Progress bars can cause problems in Jupyter notebooks so we disable them here (training does not take super long)\n",
    "config[\"trainer_kwargs/enable_progress_bar\"] = False\n",
    "\n",
    "# Uncomment the following lines if you want to use one of the pretrained models as basis for our training\n",
    "# config[\"model/pretrained_model\"] = {\n",
    "#     \"model\": \"image\",\n",
    "#     \"run_folder\": \"2022-02-03_22-58-44_generated_default_model_comparison\",\n",
    "#\n",
    "\n",
    "config_path = external/'data'/ (name + \"_config.json\")\n",
    "config.save_config(config_path)\n",
    "JSON(config_path)\n",
    "\n",
    "print(config_path)\n",
    "print(label_mapping_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start the Training\n",
    "You are now ready to train your network. open the file in this tutorial directory named \"training.sh\", and modify the config path variable to the path you just generated in the previous cell (it should be printed at the bottom).\n",
    "\n",
    "Then, in a terminal, from the root directory of the repository, run:\n",
    "```bash\n",
    " chmod +x tutorials/Urology_group_tutorials/training.sh\n",
    " sh tutorials/Urology_group_tutorials/training.sh\n",
    "```\n",
    "\n",
    "now your training has started! depending on your number of epochs, it will take time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Viewing Results\n",
    "\n",
    "Once your training is complete, you can use htc code to view experimental analysis of your model.\n",
    "start by finding and confirming location of your training directory. Navigate to your results directory (the one you set with the PATH environment variable.)\n",
    "inside results, you can find your run in a path similar to the once below:\n",
    "```bash\n",
    " training/<model_name>/<run_name>\n",
    "```\n",
    "Where the run name is usually the timestamp of the training with the name of the config used appended. it should contain a config.json, data.json, log.txt, and a fold directory for each fold you performed. if everything is there, run the following s, replacing the input path with the absolute path to the run directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m[\u001b[0m\u001b[33mWARNING\u001b[0m\u001b[1m]\u001b[0m\u001b[1m[\u001b[0m\u001b[3mdotenv.main\u001b[0m\u001b[1m]\u001b[0m Python-dotenv could not parse statement        \u001b[2mmain.py:28\u001b[0m\n",
      "starting at line \u001b[37m4\u001b[0m                                                    \u001b[2m          \u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[32mINFO\u001b[0m\u001b[1m]\u001b[0m\u001b[1m[\u001b[0m\u001b[3mhtc\u001b[0m\u001b[1m]\u001b[0m Will generate results for the following  \u001b[2mrun_table_generation.py:453\u001b[0m\n",
      "runs:                                                \u001b[2m                           \u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[32mINFO\u001b[0m\u001b[1m]\u001b[0m\u001b[1m[\u001b[0m\u001b[3mhtc\u001b[0m\u001b[1m]\u001b[0m                                          \u001b[2mrun_table_generation.py:455\u001b[0m\n",
      "median_pixel/\u001b[37m2024\u001b[0m-\u001b[37m07\u001b[0m-25_17-\u001b[37m43\u001b[0m-32_Atlas_config        \u001b[2m                           \u001b[0m\n",
      "\u001b[2K\u001b[36mCheck for necessary files\u001b[0m \u001b[90mâââââââââââââââââââââââââââââââ\u001b[0m \u001b[35m  0%\u001b[0m \u001b[36m-:--:--\u001b[0m \u001b[33m0:00:00\u001b[0m \u001b[31m?\u001b[0m\u001b[1m[\u001b[0m\u001b[33mWARNING\u001b[0m\u001b[1m]\u001b[0m\u001b[1m[\u001b[0m\u001b[3mhtc\u001b[0m\u001b[1m]\u001b[0m The log of the fold                   \u001b[2mrun_table_generation.py:196\u001b[0m\n",
      "\u001b[35m/omics/groups/OE0645/internal/data/htcdata/medium_te\u001b[0m \u001b[2m                           \u001b[0m\n",
      "\u001b[35mst/results/training/median_pixel/2024-07-25_17-43-32\u001b[0m \u001b[2m                           \u001b[0m\n",
      "\u001b[35m_Atlas_config/\u001b[0m\u001b[95mfold_1\u001b[0m contains warnings               \u001b[2m                           \u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[33mWARNING\u001b[0m\u001b[1m]\u001b[0m\u001b[1m[\u001b[0m\u001b[3mhtc\u001b[0m\u001b[1m]\u001b[0m The log of the fold                   \u001b[2mrun_table_generation.py:196\u001b[0m\n",
      "\u001b[35m/omics/groups/OE0645/internal/data/htcdata/medium_te\u001b[0m \u001b[2m                           \u001b[0m\n",
      "\u001b[35mst/results/training/median_pixel/2024-07-25_17-43-32\u001b[0m \u001b[2m                           \u001b[0m\n",
      "\u001b[35m_Atlas_config/\u001b[0m\u001b[95mfold_2\u001b[0m contains warnings               \u001b[2m                           \u001b[0m\n",
      "\u001b[2K\u001b[36mCheck for necessary files\u001b[0m \u001b[90mâââââââââââââââââââââââââââââââ\u001b[0m \u001b[35m100%\u001b[0m \u001b[36m0:00:00\u001b[0m \u001b[33m0:00:00\u001b[0m \u001b[31m?\u001b[0m\n",
      "\u001b[2K\u001b[36mCreate validation table\u001b[0m \u001b[90mâââââââââââââââââââââââââââââââââ\u001b[0m \u001b[35m100%\u001b[0m \u001b[36m0:00:00\u001b[0m \u001b[33m0:00:00\u001b[0m \u001b[31m?\u001b[0m\n",
      "\u001b[2K\u001b[36mCreate test table\u001b[0m \u001b[90mâââââââââââââââââââââââââââââââââââââââ\u001b[0m \u001b[35m100%\u001b[0m \u001b[36m0:00:00\u001b[0m \u001b[33m0:00:00\u001b[0m \u001b[31m?\u001b[0m\n",
      "\u001b[2K\u001b[36mValidate tables\u001b[0m \u001b[90mââââââââââââââââââââââââââââââââââââââââ\u001b[0m \u001b[35m  0%\u001b[0m \u001b[36m-:--:--\u001b[0m \u001b[33m0:00:00\u001b[0m \u001b[31m?\u001b[0m\u001b[1m[\u001b[0m\u001b[33mWARNING\u001b[0m\u001b[1m]\u001b[0m\u001b[1m[\u001b[0m\u001b[3mhtc\u001b[0m\u001b[1m]\u001b[0m The environment variable                          \u001b[2mDatasets.py:125\u001b[0m\n",
      "PATH_Tivita_ureter_annotations was set to                        \u001b[2m               \u001b[0m\n",
      "\u001b[35m/home/l328r/htc/\u001b[0m\"\u001b[35m/omics/groups/OE0645/internal/data/htcdata/uret\u001b[0m \u001b[2m               \u001b[0m\n",
      "\u001b[35mer/\u001b[0m\u001b[95mmitk_annotations\u001b[0m but the path does not exist                  \u001b[2m               \u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[32mINFO\u001b[0m\u001b[1m]\u001b[0m\u001b[1m[\u001b[0m\u001b[3mhtc\u001b[0m\u001b[1m]\u001b[0m Labels are not checked against example   \u001b[2mrun_table_generation.py:369\u001b[0m\n",
      "images because no labels could be found in the       \u001b[2m                           \u001b[0m\n",
      "dataframe \u001b[1m(\u001b[0m\u001b[33mcolumns\u001b[0m=\u001b[1;35mIndex\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[90m'epoch_index'\u001b[0m,             \u001b[2m                           \u001b[0m\n",
      "\u001b[90m'dataset_index'\u001b[0m, \u001b[90m'image_name'\u001b[0m, \u001b[90m'subject_name'\u001b[0m,       \u001b[2m                           \u001b[0m\n",
      "       \u001b[90m'accuracy'\u001b[0m, \u001b[90m'confusion_matrix'\u001b[0m, \u001b[90m'fold_name'\u001b[0m,  \u001b[2m                           \u001b[0m\n",
      "\u001b[90m'best_epoch_index'\u001b[0m\u001b[1m]\u001b[0m,                                 \u001b[2m                           \u001b[0m\n",
      "      \u001b[33mdtype\u001b[0m=\u001b[90m'object'\u001b[0m\u001b[1m)\u001b[0m\u001b[1m)\u001b[0m                               \u001b[2m                           \u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[32mINFO\u001b[0m\u001b[1m]\u001b[0m\u001b[1m[\u001b[0m\u001b[3mhtc\u001b[0m\u001b[1m]\u001b[0m Labels are not checked against example   \u001b[2mrun_table_generation.py:369\u001b[0m\n",
      "images because no labels could be found in the       \u001b[2m                           \u001b[0m\n",
      "dataframe \u001b[1m(\u001b[0m\u001b[33mcolumns\u001b[0m=\u001b[1;35mIndex\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[90m'epoch_index'\u001b[0m,             \u001b[2m                           \u001b[0m\n",
      "\u001b[90m'dataset_index'\u001b[0m, \u001b[90m'image_name'\u001b[0m, \u001b[90m'subject_name'\u001b[0m,       \u001b[2m                           \u001b[0m\n",
      "       \u001b[90m'accuracy'\u001b[0m, \u001b[90m'confusion_matrix'\u001b[0m, \u001b[90m'fold_name'\u001b[0m,  \u001b[2m                           \u001b[0m\n",
      "\u001b[90m'best_epoch_index'\u001b[0m\u001b[1m]\u001b[0m,                                 \u001b[2m                           \u001b[0m\n",
      "      \u001b[33mdtype\u001b[0m=\u001b[90m'object'\u001b[0m\u001b[1m)\u001b[0m\u001b[1m)\u001b[0m                               \u001b[2m                           \u001b[0m\n",
      "\u001b[2K\u001b[36mValidate tables\u001b[0m \u001b[90mââââââââââââââââââââââââââââââââââââââââ\u001b[0m \u001b[35m100%\u001b[0m \u001b[36m0:00:00\u001b[0m \u001b[33m0:00:00\u001b[0m \u001b[31m?\u001b[0m\n",
      "\u001b[?25h\u001b[1m[\u001b[0m\u001b[32mINFO\u001b[0m\u001b[1m]\u001b[0m\u001b[1m[\u001b[0m\u001b[3mhtc\u001b[0m\u001b[1m]\u001b[0m Creating notebooks\u001b[33m...\u001b[0m                    \u001b[2mrun_table_generation.py:467\u001b[0m\n",
      "\u001b[2K\u001b[36mGenerate notebooks\u001b[0m \u001b[90mââââââââââââââââââââââââââââââââââââââ\u001b[0m \u001b[35m  0%\u001b[0m \u001b[36m-:--:--\u001b[0m \u001b[33m0:00:00\u001b[0m \u001b[31m?\u001b[0m\u001b[1m[\u001b[0m\u001b[32mINFO\u001b[0m\u001b[1m]\u001b[0m\u001b[1m[\u001b[0m\u001b[3mhtc.no_duplicates\u001b[0m\u001b[1m]\u001b[0m Using the notebook         \u001b[2mrun_table_generation.py:404\u001b[0m\n",
      "htc/tissue_atlas/ExperimentAnalysisValidation.ipynb  \u001b[2m                           \u001b[0m\n",
      "\u001b[2K\u001b[36mGenerate notebooks\u001b[0m \u001b[90mââââââââââââââââââââââââââââââââââââââ\u001b[0m \u001b[35m100%\u001b[0m \u001b[36m0:00:00\u001b[0m \u001b[33m0:00:17\u001b[0m \u001b[31m?\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!python htc/evaluation/run_table_generation.py --notebook htc/tissue_atlas/ExperimentAnalysisValidation.ipynb --input-path /omics/groups/OE0645/internal/data/htcdata/medium_test/results/training/median_pixel/2024-07-25_17-43-32_Atlas_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the cell completes, you should see a new validation_table file and an ExperimentAnalysisValidation.html file in the the run directory. Open the html file in browser to see the analysis of your experiment.\n",
    "\n",
    "Bare in mind that this will only analyze the validation set. if you are satisfied with your model and want testing results, you must generate test predictions and run the same above code, but using ExperimentAnalysis.ipynb\n",
    "\n",
    "Happy Training!!! : )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "htc-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
