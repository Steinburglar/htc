import argparse
import copy
import itertools
import json
import re
from functools import cached_property
from multiprocessing import Process
from pathlib import Path
from typing import Any, Callable, Union
import numpy as np
import pandas as pd
from PIL import Image
from rich.progress import track
from typing_extensions import Self, Generator
from htc.tivita.DataPathAtlas2 import DataPathAtlas
from htc.data_processing.run_l1_normalization import L1Normalization
from htc.data_processing.run_median_spectra import MedianSpectra
from htc.data_processing.run_parameter_images import ParameterImages
from htc.data_processing.run_raw16 import Raw16
from htc.data_processing.run_rgb_sensor_aligned import RGBSensorAligned
from htc.settings import settings
from htc.tivita.DataPath import DataPath
from htc.dataset_preparation.DatasetGenerator import DatasetGenerator
from htc.tivita.DatasetSettings import DatasetSettings
from htc.tivita.metadata import generate_metadata_table
from htc.utils.AdvancedJSONEncoder import AdvancedJSONEncoder
from htc.utils.general import clear_directory, safe_copy
from htc.utils.mitk.mitk_masks import nrrd_mask
from htc.utils.parallel import p_map
from htc.utils.paths import filter_min_labels

class DatasetGeneratorAlex(DatasetGenerator):
    def __init__(self, output_path: Path, input_path: Path = None, paths: list[DataPath] | None = None, regenerate: bool = False, include_csv: bool = False):
    
        """
        Class to process data from Alex in its original form. 
        Like other DatasetGenerator classes, this class uses hte base parent class and is designed to process
        a dataset given from the clinicians (alex). However, Unlike the other DatasetGenerators,
        This class DOES NOT COPY the dataset into the new structure. Instead, it leaves it in place and creates a new
        location only for the new, generated/preprocessed data/information. that eternal directory can be then
        defined as an environment variable, which preserves the functionality of the original htc approach

        Args:
            output_path: Path where the intermediates and dataset_settings should be stored (data and intermediates subfolder will be created at this location).
            input_path: Path to the original dataset from the clinicians (e.g. folders with the name `Cat_*`).
            ****for now, folders with the name "data"**
            paths: Explicit list of paths which should be considered in the methods. If None, all found images in the output path will be used (usually what you want).
            regenerate: If True, will delete existing intermediate files before generating new ones.
            include_csv: If True, will save tables additionally as CSV files.
        """
    
        super().__init__(output_path, input_path, paths, regenerate, include_csv)
        
    
    
    @cached_property
    def dataset_paths(self) -> dict[str, list[DataPath]]:
        """
        Dictionary with dataset name as key and list of paths as value. Per default, this is a dictionary with only one entry (the main dataset) and paths of all images in the dataset. However, if the dataset contains subdatasets (e.g. 2021_02_05_Tivita_multiorgan_semantic and context_experiments), then this dictionary can be used to define the paths per subdataset (needed for the median table per subdataset). For the semantic dataset, this would look like:
        ```python
        {
            "2021_02_05_Tivita_multiorgan_semantic": [path1, path2, ...],
            "context_experiments": [path1, path2, ...],
        }
        ```
        designed to allow for using multiple "subdatasets" at once (i.e, multiple different directories
        in the original atlas structure that contain the directory "data")
        
        Iterates through the input_path, which for now should be a path to a "data" folder.
        I will add the functionality to deal with multiple "data" folders later.
        
        
        In those cases, the generator class should overwrite the `dataset_paths` property.
        """
        dict = {}
        def find_data_directories(root_path):
            root = Path(root_path)
            data_directories = list(root.rglob('data'))
    
            # Filter to ensure only directories named "data" are included
            data_directories = [p for p in data_directories if p.is_dir()]
    
            return data_directories
        
        data_subdirectories = find_data_directories(self.input_path)
        for data_dir in data_subdirectories:
            paths = list(DataPathAtlas.iterate(data_dir))
            name0 = paths[0].subdataset_name
            assert all(path.subdataset_name == name0 for path in paths), "Not all items are from the same subdataset"
            dict[name0] = paths #set the ist of paths to
            
        return dict
        
    
    def compile_image_dict(self):
        """
        method to take the dataset_paths dictionary, and produce a dictionary with the unique image names as keys,
        and the Paths leading to that image name as values. This is intended as a tool to deal with duplicates in the larger dataset, particular ones
        found in different "data" folders within the larger dataset
        
        Returns:
            Dict: dictionary of key-value pairs - str: list[DataPath] - the string is the unique image_name of the image. the list is a list
            of datapath objects to each of the different locations where the duplicate exists. 
        """
        images_dict = {}
        for path in self.paths:
            name = path.image_name()
            images_dict.setdefault(name, []).append(name) #check to see if a list has already been created. if not, initialize it
        return images_dict
    
    def subdatasets(self):
        """another heping method to organize the dataset. takes in a list of paths, and creates a dictionary
        with key value pairs:
            "name_of_subdataset" : Paths in that subdataset
        here, a subdataset is defined as a directory immediately containing a "data" folder in the original
        unstructured dataset.
        should e used INSTEAD of compile_image_dict, depending on context, because they both divide along the same
        scope, just using different criteria. 
        """
        subdatasets = {}
        for path in self.paths:
            name = path.subdataset_name #here, name is the name of the subdataset folder
            subdatasets.setdefault(name, []).append(path)#appends to list for that subdataset (createsif it does not yet exist)
        return subdatasets
    
    
    def segmentations(self):
        """
        method to iterate through all the hyperGUI folders in all the DataPaths available (across all subdatasets),
        and compress them into a blosc dictionary in the standard format of the intermediates directory. 
        """
            
                
    
    
    
    
    
    
    
    
    
    
    
    
    def _yield_hypergui_data(
        self,
        paths: list[DataPath],
        hypergui_mapping: dict[str, dict[str, str]],
        )   -> Generator[dict, None, None]:
        """
        Yield data and annotations from a folder with hypergui annotations. This method processes the image data, generates PNG mask files for the annotations, and collects the meta label JSON information.

        Args:
            paths: List of image paths which all show the same image but annotate different parts in the image (since an image may be stored more than once in the original dataset).
            hypergui_mapping: Information for every _hypergui_N folder. For each folder, a dictionary with the keys `annotation_type` (e.g. polygon or circle) and `annotator` (e.g. annotator1) must be provided.

        Yields:
            dict: A dictionary containing the processed data for each path, including masks, meta labels, and any relevant annotations.
        """
        assert all(
            p.timestamp == paths[0].timestamp for p in paths
        ), "The timestamp for all image paths must be the same (identical image data)"

        # Angle mapping for the standardized dataset (only used if available)
        angle_mapping = {
            1: 0,
            2: -25,
            3: 25,
        }

        self.missing_binaries = []
        self.missing_coordinates = []

        ignored_prefixes = (".", "_", "Thumbs.db")

        def _extend_meta_labels(label: str, meta_labels: dict, p: DataPath) -> None:
            if label not in meta_labels["image_labels"]:
                meta_labels["image_labels"].append(label)
                meta_labels["image_labels"] = sorted(set(meta_labels["image_labels"]))

            for label_file in sorted(p().glob("_labelling*")):
                if "paper_tags" not in meta_labels:
                    meta_labels["paper_tags"] = []

                if label_file.stem not in meta_labels["paper_tags"]:
                    meta_labels["paper_tags"].append(label_file.stem)

                # Convert standardized information to a structured format (if available)
                match = re.search(r"_labelling_standardized_situs_(\d+)", label_file.stem)
                if match is not None:
                    situs = int(match.group(1))

                    if "label_meta" not in meta_labels:
                        meta_labels["label_meta"] = {}
                    label_meta = meta_labels["label_meta"]
                    if label not in label_meta:
                        label_meta[label] = {}
                    current_label_meta = label_meta[label]

                    if "situs" in current_label_meta:
                        assert (
                            current_label_meta["situs"] == situs
                        ), f"Found different situs for the label: {label} (path {p})"
                    else:
                        current_label_meta["situs"] = situs

                    match = re.search(
                        r"_labelling_standardized_situs_(\d+)_angle_(\d+)_repetition_(\d+)", label_file.stem
                    )
                    if match is not None:
                        angle = angle_mapping[int(match.group(2))]
                        repetition = int(match.group(3))
                        if "angle" in current_label_meta:
                            assert current_label_meta["angle"] == angle, f"Found different angle for the label: {label}"
                        else:
                            current_label_meta["angle"] = angle
                        if "repetition" in current_label_meta:
                            assert (
                                current_label_meta["repetition"] == repetition
                            ), f"Found different repetition for the label: {label}"
                        else:
                            current_label_meta["repetition"] = repetition
        
            for p in paths:
                meta_labels = {"image_labels": []}

                # The path is only associated with one label
                if hasattr(p, "organ"):
                    label = p.organ
                    _extend_meta_labels(label, meta_labels, p)

                for hypergui_dir in sorted(p().iterdir()): #goes through and assigns labels according to Hypergui mapping
                    #might have to also change if there are multiple subdatasets under the same label.
                    if hypergui_dir.is_dir() and hypergui_dir.name.startswith("_hypergui"):
                        if hypergui_dir.name not in hypergui_mapping:
                            settings.log.warning(
                                f"Found an unknown hypergui folder: {hypergui_dir}. The folder will be ignored"
                            )
                            continue

                        annotation_info = hypergui_mapping[hypergui_dir.name]
                        if "label_name" in annotation_info:
                            # The path is associated with multiple labels (each label with their own _hypergui_* folder)
                            label = annotation_info["label_name"]
                            _extend_meta_labels(label, meta_labels, p) #this updates the Datapath objects metadata to include the label

                        binary_path = hypergui_dir / "mask.csv"
                        if binary_path.exists():
                            mask = pd.read_csv(binary_path, header=None).to_numpy()
                            mask = mask.astype(bool)
                            mask = Image.fromarray(mask).convert("1")
                        else:
                            self.missing_binaries.append(hypergui_dir)
                            mask = None

                        coordinates_path = hypergui_dir / "MASK_COORDINATES.csv"
                        if not coordinates_path.exists():
                            self.missing_coordinates.append(hypergui_dir)
                            coordinates_path = None
                        yield {
                            "path": p,
                            "meta_labels": meta_labels,
                            "timestamp": p.timestamp,
                            "annotation_type": annotation_info["annotation_type"],
                            "annotator": annotation_info["annotator"],
                            "label": label,
                            "mask": mask,
                            "coordinates_path": coordinates_path,
                        }


    def _check_hypergui_data(self) -> None:
        assert (
            self.missing_binaries is not None and self.missing_coordinates is not None
        ), "Please call _copy_hypergui_paths() before running this check"

        if len(self.missing_binaries) > 0 or len(self.missing_coordinates) > 0:
            settings.log.warning(f"There are {len(self.missing_binaries)} _hypergui folders without a mask.csv file")
            settings.log.warning(
                f"There are {len(self.missing_coordinates)} _hypergui folders without a MASK_COORDINATES.csv file"
            )

            bin_coord = set(self.missing_binaries) - set(self.missing_coordinates)
            msg = f"{len(bin_coord)} folders with binaries but no coordinates:\n"
            for f in bin_coord:
                msg += f"\t{f}\n"

            coord_bin = set(self.missing_coordinates) - set(self.missing_binaries)
            msg += f"{len(coord_bin)} folders with coordinates but no binaries:\n"
            for f in coord_bin:
                msg += f"\t{f}\n"

            intersection = set(self.missing_binaries).intersection(set(self.missing_coordinates))
            msg += f"{len(intersection)} folders self.missing both:\n"
            for f in intersection:
                msg += f"\t{f}\n"
            settings.log.warning(msg)

    def _hypergui_label_mapping(self) -> tuple[dict[str, int], int]:
        """
        Crawls the dataset and searches for all annotated labels, i.e. labels with annotations (png files). Labels which only occur as image labels are not considered.

        Returns: A label mapping and the index of the last valid label (i.e. the last label which is neither overlap nor unlabeled).
        """
        # Get all labels in the dataset
        all_labels = set()
        for path in track(self.paths, description="Collect labels", refresh_per_second=1):
            image_labels = path.meta("image_labels")
            assert image_labels is not None, f"There must always be at least image labels ({path})"

            # Only labels that are annotated in the image are relevant for the label mapping
            for png_file in sorted((path() / "annotations").glob("*.png")):
                timestamp, annotation_type, annotator, label_name, file_type = png_file.stem.split("#")
                assert timestamp == path.timestamp
                assert annotation_type == "polygon"
                assert label_name in image_labels, (
                    f"The label {label_name} is annotated for the image {path} but the label is not part of the"
                    f" image label list: {image_labels}"
                )
                assert file_type == "binary"

                all_labels.add(label_name)

            for nrrd_file in sorted((path() / "annotations").glob("*.nrrd")):
                timestamp, annotation_type, annotator = nrrd_file.stem.split("#")
                assert timestamp == path.timestamp

                mitk_data = nrrd_mask(nrrd_file)
                labels = mitk_data["label_mapping"].label_names()
                assert set(labels).issubset(
                    image_labels
                ), f"Labels {labels} are not part of the image labels: {image_labels}"
                all_labels.update(labels)

        # Generate a mapping based on the sorted label list
        all_labels = sorted(all_labels)
        label_mapping = dict(zip(all_labels, range(len(all_labels))))
        assert len(label_mapping) < 254, "Too many labels"

        last_valid_label_index = len(label_mapping) - 1
        label_mapping["overlap"] = 254
        label_mapping["unlabeled"] = 255

        return label_mapping, last_valid_label_index
        