{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ureter Training Walkthrough\n",
    "This notebook will walk a user through using the Atlas compatible htc for training their own segmentation model on ureter data. This notebook is almost a direct copy of the SegmentationTraining notebook in the tutorials, but with a couple things changed for compatability with ureter data. (the ureter data comes in a slightly different format and thus requires some changes)\n",
    "\n",
    "\n",
    "Start with necessary inputs and define path to your dataset_settings .json. Replace relevant directory paths / names with the names to your own dataset and json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/omics/groups/OE0645/internal/data/htcdata/ureter/external/intermediates\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "from IPython.display import JSON\n",
    "from typing import TYPE_CHECKING, Any, Callable, Union, Self\n",
    "from htc import (\n",
    "    Config,\n",
    "    DataPath,\n",
    "    DataSpecification,\n",
    "    MetricAggregation,\n",
    "    SpecsGeneration,\n",
    "    create_class_scores_figure,\n",
    "    settings,\n",
    "    LabelMapping\n",
    ")\n",
    "from htc.models.data.SpecsGenerationUreter import SpecsGenerationUreter\n",
    "\n",
    "intermediates_dir = settings.intermediates_dirs.external\n",
    "print(intermediates_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can specify important parameters for your training run, such as fold, train/test split, etc. replace the values in the following code block with the values of your choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TO DO:\n",
    "#filter by existence of .txt file next to hypergui within timestamp folder -- lets you know that its ok\n",
    "#add batch size\n",
    "#add epoch length\n",
    "#add batch randomization conditions\n",
    "\n",
    "filter_mitk = lambda p: p.contains_MITK()\n",
    "filters = [filter_mitk] #list of callable filter functions, can be a variety of things\n",
    "annotation_name = 'annotator1' #name of annotators to be used\n",
    "test_ratio = 0.334 #ratio of images to be saved as test, i.e, not ued in any training. should be float between 0.0 and 1.0\n",
    "n_folds = 2 #number of folds to make in the training data. training data (not test data) will be randomly split into n_folds different groups\n",
    "#for each \"fold\", the network will train a model with one of the groups as validation and all the other groups as training data.  \n",
    "seed = None #optional parameter that interacts with the random grouping of the folding operation. For a different fold upon every function call, set = None.\n",
    "# for a consistent fold, set seed to a number of your choice, e.g. seed = 42\n",
    "name = \"leftrightUreterSegment\" #name of a json file created in the following code block, that gets stored in the parent directory of this notebook. name it something simple and descriptive\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/omics/groups/OE0645/internal/data/htcdata/ureter/external\n",
      "['patient_28', 'patient_78', 'patient_85', 'patient_73', 'patient_75', 'patient_38', 'patient_79', 'patient_34', 'patient_72', 'patient_58', 'patient_64', 'patient_42', 'patient_26', 'patient_74', 'patient_13', 'patient_49', 'patient_54', 'patient_56', 'patient_25', 'patient_17', 'patient_21', 'patient_48', 'patient_27', 'patient_60', 'patient_67', 'patient_6', 'patient_12', 'patient_37', 'patient_59', 'patient_70', 'patient_84', 'patient_52', 'patient_63', 'patient_51', 'patient_30', 'patient_57', 'patient_55']\n"
     ]
    }
   ],
   "source": [
    "tutorial_dir = Path().absolute()\n",
    "external = settings.external_dir.external['path_dataset']#need brackets to acess the path, because settings.external_dir.external is a dictionary cointaing info about the external_dir.\n",
    "#settings.external_dir is an object containing all the different external directories: in our case, there should always just be one with shortcut \"external\"\n",
    "print(external)\n",
    "specs_path = external/'data' / name\n",
    "SpecsGenerationUreter(intermediates_dir,\n",
    "                filters = filters,\n",
    "                annotation_name = annotation_name,\n",
    "                test_ratio = test_ratio,\n",
    "                n_folds = n_folds,\n",
    "                seed = seed,\n",
    "                name = name,\n",
    "                ).generate_dataset(external / 'data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lightning Class\n",
    "\n",
    "Next step is to choose/build our lightning class. The Lightning class (as in Pytorch Lightning) performs many aspects of managing training, and can b customized by creating your own child class. most notably, the Lightning class allows you to specify your Loss function.\n",
    "\n",
    "For this walkthrough, we will use the htc default \"LightingImage\" class, which is their default class for training on full images (as opposed to patches, pixels, or superpixels). This calculates loss as a weighted average of Dice loss and Cross-Entropy loss. See the htc \"networkTraining\" tutorial for more info on the lightning class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config\n",
    "The last step before training is to create our configuration file. This file is also a json that contains important metadata, and it is used by the training process itself to configure training hyperparameters, like batch size and transformations. We will use the htc's Config ***class*** to write the config ***json***\n",
    "\n",
    "The following Code block will write the config json for you. By default, it will store the config.json file in the same directory as your dataset_settings json.\n",
    "\n",
    "The following code block is still set up for tutorial use, not production use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assign training hyperparameters\n",
    "max_epochs = 10000 #this can be whatever you want\n",
    "batch_size = 8 #this is the number of SUBJECTS, rather than images, in each batch. The loader is designed to sort batches by subject\n",
    "#default batch size is 3 subjects\n",
    "shuffle = True #this tells the batch generator to retrieve random, different, batches on every epoch.\n",
    "#True causes it to be random, False will leave same batches across epochs. \n",
    "num_workers = \"auto\" #how many dataloading \"worker\" subprocesses to start. The optimal amount for fast loading is highly dependant on your system\n",
    "#you can experiment on low-epoch runs to see what num_workers maximizes your training speed. \n",
    "#left to implement: specialized sampling practices? such as guaranteeing even organ distribution across classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pathlib.PosixPath'>\n"
     ]
    }
   ],
   "source": [
    "config = Config.from_model_name(\"default\", \"image\")\n",
    "config[\"inherits\"] = \"htc/context/models/configs/organ_transplantation_0.8.json\" #using organ translplantation model\n",
    "config[\"input/data_spec\"] = specs_path\n",
    "config[\"input/annotation_name\"] = [\"mitk#annotator1\"]\n",
    "config[\"validation/checkpoint_metric_mode\"] = \"class_level\"\n",
    "\n",
    "\n",
    "\n",
    "# We want to merge the annotations from all annotators into one label mask\n",
    "config[\"input/merge_annotations\"] = \"union\"\n",
    "\n",
    "# We have a two-class problem and we want to ignore all unlabeled pixels\n",
    "# Everything which is >= settings.label_index_thresh will later be considered invalid\n",
    "config[\"label_mapping\"] = {\n",
    "        \"last_valid_label_index\": 4,\n",
    "        \"mapping_index_name\": {\n",
    "            \"1\": \"background_anorganic\",\n",
    "            \"2\": \"background_organic\",\n",
    "            \"3\": \"ureter_left\",\n",
    "            \"4\": \"ureleter_right\"\n",
    "            },\n",
    "        \"mapping_name_index\": {\n",
    "            \"background_anorganic\": 1, \n",
    "            \"background_organic\": 2,\n",
    "            \"ureter_left\": 3,\n",
    "            \"ureter_right\": 4,\n",
    "            },\n",
    "        \"unknown_invalid\": True,\n",
    "        \"zero_is_invalid\": True}\n",
    "\n",
    "\n",
    "#specify batch and sampler settings:\n",
    "config['dataloader_kwargs/batch_size'] = batch_size\n",
    "config['dataloader_kwargs/num_workers'] = num_workers\n",
    "\n",
    "# Reduce the training time\n",
    "config[\"trainer_kwargs/max_epochs\"] = max_epochs\n",
    "\n",
    "# Progress bars can cause problems in Jupyter notebooks so we disable them here (training does not take super long)\n",
    "config[\"trainer_kwargs/enable_progress_bar\"] = True\n",
    "\n",
    "# Uncomment the following lines if you want to use one of the pretrained models as basis for our training\n",
    "# config[\"model/pretrained_model\"] = {\n",
    "#     \"model\": \"image\",\n",
    "#     \"run_folder\": \"2022-02-03_22-58-44_generated_default_model_comparison\",\n",
    "#\n",
    "\n",
    "config_path = external/'data'/ (name + \"_config.json\")\n",
    "config.save_config(config_path)\n",
    "JSON(config_path)\n",
    "\n",
    "print(type(config_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start the Training\n",
    "You are now ready to train your network. Simply run the `htc training` command and pass the model type (image model in our case) and path to the config as arguments.\n",
    "> &#x26a0;&#xfe0f; Starting a training session in a Jupyter notebook is usually not a good idea. Instead, it is advisable to use a [`screen`](https://linuxize.com/post/how-to-use-linux-screen/) environment so that your training runs in the background and you can return later to check for the status.\n",
    "\n",
    "> There is also a `--fold FOLD_NAME` switch if you only want to train only one fold. This is useful for debugging."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "htc-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
